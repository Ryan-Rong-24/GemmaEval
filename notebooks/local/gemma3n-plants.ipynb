{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wh4ZpnndMvaO"
   },
   "source": [
    "# Gemma 3N Plant Dataset Fine-tuning\n",
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
    "</div>\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Selection - Set which GPU to use\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # Use GPU 3 instead of GPU 0\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # Use GPU 2\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  # Use GPU 3\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"  # Use GPUs 1 and 2\n",
    "\n",
    "print(f\"CUDA_VISIBLE_DEVICES set to: {os.environ.get('CUDA_VISIBLE_DEVICES', 'default')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBia8GVKMvaS"
   },
   "source": [
    "### Plant Dataset Fine-tuning Configuration\n",
    "\n",
    "**Key Features:**\n",
    "- üåø **Plant Identification**: Fine-tune on plant dataset for species identification\n",
    "- üéØ **Configurable Dataset Size**: Set `MAX_IMAGES_PER_SPECIES` to control training data size\n",
    "- üñºÔ∏è **Vision Fine-tuning**: Enabled vision layers for multimodal learning\n",
    "- üìä **Balanced Species**: Automatically balances across different plant species\n",
    "- üîÑ **Adaptive Training**: Training steps adjust automatically based on dataset size\n",
    "\n",
    "**Configuration Parameters:**\n",
    "- `DATASET_PATH`: Path to your plant dataset\n",
    "- `MAX_IMAGES_PER_SPECIES`: Number of images per species (default: 200)\n",
    "- `IDENTIFICATION_PROMPT`: The prompt used for plant identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3z2yeVsMvaV"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T18:28:34.650879Z",
     "iopub.status.busy": "2025-07-06T18:28:34.650642Z",
     "iopub.status.idle": "2025-07-06T18:28:48.346821Z",
     "shell.execute_reply": "2025-07-06T18:28:48.346058Z",
     "shell.execute_reply.started": "2025-07-06T18:28:34.650856Z"
    },
    "id": "pS-P_shPMvaX",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !uv pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv pip install --no-deps --upgrade timm # Only for Gemma 3N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip show transformers triton torch xformers timm unsloth unsloth-zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T18:28:53.456574Z",
     "iopub.status.busy": "2025-07-06T18:28:53.455877Z",
     "iopub.status.idle": "2025-07-06T18:28:53.833215Z",
     "shell.execute_reply": "2025-07-06T18:28:53.832653Z",
     "shell.execute_reply.started": "2025-07-06T18:28:53.456539Z"
    },
    "id": "BRtQrJjt2_wG",
    "outputId": "28e6c447-542e-42da-9c41-ecfb3183a6ca",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Verify the plant dataset structure\n",
    "import os\n",
    "import glob\n",
    "# dataset_base_path = \"/content/drive/MyDrive/plants\"\n",
    "dataset_base_path = '../../data/plants/train/'\n",
    "\n",
    "print(\"Checking plant dataset structure...\")\n",
    "if os.path.exists(dataset_base_path):\n",
    "    print(f\"‚úÖ Found plant folder at: {dataset_base_path}\")\n",
    "    \n",
    "    # Check for species folders\n",
    "    species_folders = [d for d in os.listdir(dataset_base_path) if os.path.isdir(os.path.join(dataset_base_path, d))]\n",
    "    \n",
    "    if species_folders:\n",
    "        print(f\"‚úÖ Found {len(species_folders)} species folders:\")\n",
    "        total_images = 0\n",
    "        for species in sorted(species_folders):\n",
    "            species_path = os.path.join(dataset_base_path, species)\n",
    "            image_files = []\n",
    "            for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
    "                image_files.extend(glob.glob(os.path.join(species_path, ext)))\n",
    "            image_count = len(image_files)\n",
    "            total_images += image_count\n",
    "            print(f\"   - {species}: {image_count} images\")\n",
    "        \n",
    "        print(f\"‚úÖ Total images found: {total_images}\")\n",
    "    else:\n",
    "        print(\"‚ùå No species folders found\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Plant dataset not found at: {dataset_base_path}\")\n",
    "    print(\"Please ensure your plant dataset is available\")\n",
    "    print(\"Expected structure:\")\n",
    "    print(\"plant_data/\")\n",
    "    print(\"‚îú‚îÄ‚îÄ Dandelion/\")\n",
    "    print(\"‚îÇ   ‚îú‚îÄ‚îÄ image1.jpg\")\n",
    "    print(\"‚îÇ   ‚îî‚îÄ‚îÄ image2.jpg\")\n",
    "    print(\"‚îú‚îÄ‚îÄ Chickweed/\")\n",
    "    print(\"‚îÇ   ‚îú‚îÄ‚îÄ image1.jpg\")\n",
    "    print(\"‚îÇ   ‚îî‚îÄ‚îÄ image2.jpg\")\n",
    "    print(\"‚îî‚îÄ‚îÄ ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGMWlrRdzwgf"
   },
   "source": [
    "### Unsloth\n",
    "\n",
    "`FastModel` supports loading nearly any model now! This includes Vision and Text models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T18:29:29.410393Z",
     "iopub.status.busy": "2025-07-06T18:29:29.410078Z",
     "iopub.status.idle": "2025-07-06T18:31:20.039603Z",
     "shell.execute_reply": "2025-07-06T18:31:20.038756Z",
     "shell.execute_reply.started": "2025-07-06T18:29:29.410368Z"
    },
    "id": "-Xbb0cuLzwgf",
    "outputId": "ed7f7b65-b93e-4689-e2e5-6422f3115c0e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n",
    "    # Pretrained models\n",
    "    \"unsloth/gemma-3n-E4B-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3n-E2B-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other Gemma 3 quants\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer_inference = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3n-E2B-it\",\n",
    "    dtype = None, # None for auto detection\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = True,\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix PyTorch Dynamo recompile limits for Unsloth + Gemma 3N\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.cache_size_limit = 1000  # Increase from default 64\n",
    "torch._dynamo.config.suppress_errors = True   # Don't fail on compilation errors\n",
    "\n",
    "# Set up environment for better PyTorch compilation\n",
    "import os\n",
    "os.environ['TORCH_LOGS'] = 'recompiles'  # Monitor recompilations\n",
    "os.environ['TORCHDYNAMO_VERBOSE'] = '0'   # Reduce verbose output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixr4dyTHVIcI"
   },
   "source": [
    "# Gemma 3N can process Text, Vision and Audio!\n",
    "\n",
    "Let's first experience how Gemma 3N can handle multimodal inputs. We use Gemma 3N's recommended settings of `temperature = 1.0, top_p = 0.95, top_k = 64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T18:32:41.789701Z",
     "iopub.status.busy": "2025-07-06T18:32:41.788809Z",
     "iopub.status.idle": "2025-07-06T18:32:41.795987Z",
     "shell.execute_reply": "2025-07-06T18:32:41.795151Z",
     "shell.execute_reply.started": "2025-07-06T18:32:41.789672Z"
    },
    "id": "UsfUPU-oVQYu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "# Helper function for inference\n",
    "def do_gemma_3n_inference(messages, max_new_tokens = 128):\n",
    "    _ = model.generate(\n",
    "        **tokenizer_inference.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            tokenize = True,\n",
    "            return_dict = True,\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\"),\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "        streamer = TextStreamer(tokenizer_inference, skip_prompt = True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2-ddk0CWeTA"
   },
   "source": [
    "# Gemma 3N can see images!\n",
    "\n",
    "<img src=\"https://files.worldwildlife.org/wwfcmsprod/images/Sloth_Sitting_iStock_3_12_2014/story_full_width/8l7pbjmj29_iStock_000011145477Large_mini__1_.jpg\" alt=\"Alt text\" height=\"256\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T18:33:12.358957Z",
     "iopub.status.busy": "2025-07-06T18:33:12.358115Z",
     "iopub.status.idle": "2025-07-06T18:34:55.907077Z",
     "shell.execute_reply": "2025-07-06T18:34:55.906227Z",
     "shell.execute_reply.started": "2025-07-06T18:33:12.358918Z"
    },
    "id": "9jGeSb9bWe0k",
    "outputId": "3bd1c811-52d2-4817-bc15-01b11064e545",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sloth_link = \"https://files.worldwildlife.org/wwfcmsprod/images/Sloth_Sitting_iStock_3_12_2014/story_full_width/8l7pbjmj29_iStock_000011145477Large_mini__1_.jpg\"\n",
    "# Try to find a plant image for demonstration\n",
    "import glob\n",
    "import os\n",
    "\n",
    "plant_image = None\n",
    "if os.path.exists(dataset_base_path):\n",
    "    for species_folder in os.listdir(dataset_base_path):\n",
    "        species_path = os.path.join(dataset_base_path, species_folder)\n",
    "        if os.path.isdir(species_path):\n",
    "            for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
    "                images = glob.glob(os.path.join(species_path, ext))\n",
    "                if images:\n",
    "                    plant_image = images[0]\n",
    "                    print(f\"Found plant image: {plant_image}\")\n",
    "                    break\n",
    "            if plant_image:\n",
    "                break\n",
    "\n",
    "image_link = plant_image if plant_image else sloth_link\n",
    "image = Image.open(image_link)\n",
    "\n",
    "print(\"Image link:\", image_link)\n",
    "plt.imshow(image)\n",
    "\n",
    "messages = [{\n",
    "    \"role\" : \"user\",\n",
    "    \"content\": [\n",
    "        { \"type\": \"image\", \"image\" : image },\n",
    "        { \"type\": \"text\",  \"text\" : \"What type of plant is this? Can you identify the species?\" }\n",
    "    ]\n",
    "}]\n",
    "# You might have to wait 1 minute for Unsloth's auto compiler\n",
    "do_gemma_3n_inference(messages, max_new_tokens = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bw5XPyYFajyM"
   },
   "source": [
    "# Let's finetune Gemma 3N!\n",
    "\n",
    "You can finetune the vision and text parts for now through selection - the audio part can also be finetuned - we're working to make it selectable as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update a small amount of parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T18:36:52.610748Z",
     "iopub.status.busy": "2025-07-06T18:36:52.609840Z",
     "iopub.status.idle": "2025-07-06T18:37:00.720595Z",
     "shell.execute_reply": "2025-07-06T18:37:00.719679Z",
     "shell.execute_reply.started": "2025-07-06T18:36:52.610721Z"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "d273c423-d571-4937-80e1-aa9c946ed33b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True,  # Turn ON for vision tasks!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # Should leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the `Gemma-3` format for conversation style finetunes with the plant dataset. We'll create vision-text pairs for plant identification. Each sample will contain an image and the corresponding question/answer about plant species identification.\n",
    "\n",
    "Gemma-3 renders multi turn conversations like below:\n",
    "```\n",
    "<bos><start_of_turn>user\n",
    "<image>\n",
    "What type of plant is this?<end_of_turn>\n",
    "<start_of_turn>model  \n",
    "This is an ...<end_of_turn>\n",
    "```\n",
    "\n",
    "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3, phi4, qwen2.5, gemma3` and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T18:37:00.722277Z",
     "iopub.status.busy": "2025-07-06T18:37:00.722021Z",
     "iopub.status.idle": "2025-07-06T18:37:00.728249Z",
     "shell.execute_reply": "2025-07-06T18:37:00.727326Z",
     "shell.execute_reply.started": "2025-07-06T18:37:00.722258Z"
    },
    "id": "LjY75GoYUCB8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a copy of the tokenizer for training (with chat template)\n",
    "# but keep the original tokenizer_inference for multimodal inference\n",
    "import copy\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    copy.deepcopy(tokenizer_inference),  # Use a copy to preserve the original\n",
    "    chat_template = \"gemma-3\",\n",
    ")\n",
    "\n",
    "# Keep the original tokenizer_inference for proper multimodal inference\n",
    "print(\"Created training tokenizer with chat template\")\n",
    "print(\"Keeping original tokenizer_inference for multimodal inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQkXuGYxbJ-e"
   },
   "source": [
    "### Plant Dataset Configuration\n",
    "\n",
    "**Important**: Before running this notebook, first split your dataset using:\n",
    "```bash\n",
    "python split_plant_dataset.py --source data/plants --output data/plants\n",
    "```\n",
    "\n",
    "This creates `data/plants/train/` and `data/plants/test/` directories with a proper holdout test set.\n",
    "\n",
    "Configure the number of images to use for training per species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T18:37:10.893387Z",
     "iopub.status.busy": "2025-07-06T18:37:10.893108Z",
     "iopub.status.idle": "2025-07-06T18:37:10.898836Z",
     "shell.execute_reply": "2025-07-06T18:37:10.897785Z",
     "shell.execute_reply.started": "2025-07-06T18:37:10.893367Z"
    },
    "id": "Mkq4RvEq7FQr",
    "outputId": "a0825db3-d0cf-4109-9e8a-6a4da598e277",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuration for plant dataset\n",
    "DATASET_PATH = \"../../data/plants/train/\"  # Path to the training split (after running split_plant_dataset.py)\n",
    "MAX_IMAGES_PER_SPECIES = 2000  # Number of images to use per species\n",
    "INCLUDE_DETAILED_RESPONSES = False  # Include detailed feature descriptions\n",
    "\n",
    "# Train/Validation Split Configuration\n",
    "VALIDATION_SPLIT = 0.0  # 20% for validation, 80% for training\n",
    "USE_VALIDATION = False   # Set to False to use all data for training\n",
    "\n",
    "# Define the identification prompts\n",
    "IDENTIFICATION_PROMPTS = [\n",
    "    \"What type of plant is this? Please respond concisely.\",\n",
    "    \"Can you identify this plant species? Please respond concisely.\", \n",
    "    \"What species does this plant belong to? Please respond concisely.\",\n",
    "    \"Please identify this plant. Please respond concisely.\",\n",
    "    \"What kind of plant am I looking at? Please respond concisely.\",\n",
    "    \"Help me identify this plant. Please respond concisely.\"\n",
    "]\n",
    "\n",
    "# Plant species descriptions (from prepare_plant_dataset.py)\n",
    "PLANT_DESCRIPTIONS = {\n",
    "    \"Alfalfa\": {\n",
    "        \"description\": \"A perennial legume cultivated globally as a primary forage crop. It is highly valued for its high protein content and ability to improve soil by fixing nitrogen.\",\n",
    "        \"features\": [\"Compound leaves with three leaflets\", \"Clusters of small purple flowers\", \"Deep taproot system\", \"Grows up to 1 meter tall\"],\n",
    "        \"habitat\": \"Cultivated fields, grasslands, and pastures.\",\n",
    "        \"uses\": \"Forage for livestock (hay and silage); cover crop; soil improvement.\"\n",
    "    },\n",
    "    \"Asparagus\": {\n",
    "        \"description\": \"A popular perennial vegetable known for its tender, edible spears that emerge in the spring. Once established, plants can be productive for many years.\",\n",
    "        \"features\": [\"Edible spears in spring\", \"Feathery, fern-like foliage\", \"Small bell-shaped, greenish-white flowers\", \"Red berries on female plants\"],\n",
    "        \"habitat\": \"Cultivated in gardens and farms with well-drained soil.\",\n",
    "        \"uses\": \"Culinary vegetable; ornamental plant.\"\n",
    "    },\n",
    "    \"Broadleaf Plantain\": {\n",
    "        \"description\": \"An extremely common and resilient perennial herb found in disturbed areas worldwide. It is well-known in traditional medicine as a soothing poultice.\",\n",
    "        \"features\": [\"Rosette of broad, oval leaves\", \"Prominent parallel leaf veins\", \"Tall spike of inconspicuous green flowers\", \"Grows low to the ground\"],\n",
    "        \"habitat\": \"Lawns, footpaths, roadsides, and compacted or disturbed soil.\",\n",
    "        \"uses\": \"Traditional medicine (for stings, burns, and cuts); edible young leaves.\"\n",
    "    },\n",
    "    \"Cattail\": {\n",
    "        \"description\": \"A tall wetland plant easily identified by its unique brown, sausage-shaped flower spike. It is a vital resource for wildlife and has numerous survival uses.\",\n",
    "        \"features\": [\"Distinctive brown cylindrical flower head\", \"Long, flat, blade-like leaves\", \"Grows in dense colonies in water\", \"Sturdy, tall stalk\"],\n",
    "        \"habitat\": \"Marshes, ponds, ditches, and shallow freshwater edges.\",\n",
    "        \"uses\": \"Edible shoots and rhizomes; weaving material; fire tinder; wildlife habitat.\"\n",
    "    },\n",
    "    \"Chicory\": {\n",
    "        \"description\": \"A hardy perennial with vibrant blue flowers, often seen along roadsides. Its root is famously roasted and used as a coffee substitute or additive.\",\n",
    "        \"features\": [\"Bright, sky-blue daisy-like flowers\", \"Tough, grooved, and branching stem\", \"Toothed basal leaves (similar to a dandelion)\", \"Flowers often close in the afternoon\"],\n",
    "        \"habitat\": \"Roadsides, pastures, and disturbed, sunny ground.\",\n",
    "        \"uses\": \"Roasted root as a coffee substitute; edible leaves (radicchio is a variety); forage.\"\n",
    "    },\n",
    "    \"Coneflower\": {\n",
    "        \"description\": \"A popular North American prairie native (genus Echinacea) widely grown as an ornamental flower and as a major commercial herbal supplement.\",\n",
    "        \"features\": [\"Drooping purple or pink petals\", \"Spiny, cone-shaped center\", \"Hairy stems and leaves\", \"Daisy-like appearance\"],\n",
    "        \"habitat\": \"Native to prairies and open woodlands; widely cultivated in gardens.\",\n",
    "        \"uses\": \"Popular herbal supplement (Echinacea); ornamental garden plant; vital nectar source for pollinators.\"\n",
    "    },\n",
    "    \"Dandelion\": {\n",
    "        \"description\": \"A ubiquitous perennial herb, often seen as a weed but also a nutritious food and a critical early-season food source for pollinators.\",\n",
    "        \"features\": [\"Bright yellow composite flower\", \"Deeply toothed, basal leaves\", \"Hollow stem with milky sap\", \"Puffy white seed head\"],\n",
    "        \"habitat\": \"Lawns, fields, roadsides, and disturbed ground worldwide.\",\n",
    "        \"uses\": \"Edible leaves, flowers, and roots; traditional medicine; important for pollinators.\"\n",
    "    },\n",
    "    \"Elderberry\": {\n",
    "        \"description\": \"A deciduous shrub known for its large clusters of white flowers and dark purple berries. Both are widely used in culinary and medicinal preparations.\",\n",
    "        \"features\": [\"Large, flat-topped clusters of creamy-white flowers\", \"Drooping clusters of small, dark purple-black berries\", \"Compound leaves with 5-9 leaflets\", \"Shrub or small tree form\"],\n",
    "        \"habitat\": \"Woodlands, hedgerows, stream banks, and disturbed areas.\",\n",
    "        \"uses\": \"Berries for syrups, jams, and wine; flowers for cordials; popular cold remedy; wildlife food.\"\n",
    "    },\n",
    "    \"Japanese Knotweed\": {\n",
    "        \"description\": \"A large, highly aggressive herbaceous perennial considered one of the world's most destructive invasive species. Its strong rhizomes can damage infrastructure.\",\n",
    "        \"features\": [\"Hollow, bamboo-like stems with reddish speckles\", \"Large, spade-shaped leaves\", \"Plumes of small, creamy-white flowers\", \"Forms dense, impenetrable thickets\"],\n",
    "        \"habitat\": \"Riverbanks, roadsides, gardens, and waste areas; thrives in disturbed soil.\",\n",
    "        \"uses\": \"Classified as a harmful invasive pest; can damage foundations and pavement.\"\n",
    "    },\n",
    "    \"Kudzu\": {\n",
    "        \"description\": \"A notoriously invasive perennial vine from Asia that grows with extreme speed, blanketing trees, buildings, and landscapes in the southeastern United States.\",\n",
    "        \"features\": [\"Extremely rapid vine growth\", \"Large compound leaves with three broad leaflets\", \"Purple, fragrant flowers in late summer\", \"Completely smothers other vegetation\"],\n",
    "        \"habitat\": \"Forests, fields, and roadsides; climbs over any available structure.\",\n",
    "        \"uses\": \"Considered a destructive invasive pest; sometimes used for erosion control or livestock fodder.\"\n",
    "    },\n",
    "    \"Lambs Quarters\": {\n",
    "        \"description\": \"A common annual weed that is also a highly nutritious wild edible, closely related to spinach and quinoa. New growth often has a powdery white coating.\",\n",
    "        \"features\": [\"Diamond-shaped or triangular leaves\", \"White, mealy powder on new leaves and underside\", \"Inconspicuous green flower clusters\", \"Erect, branching stem\"],\n",
    "        \"habitat\": \"Gardens, farms, and disturbed, nutrient-rich soil.\",\n",
    "        \"uses\": \"Nutritious edible green (cooked); animal fodder.\"\n",
    "    },\n",
    "    \"Mullein\": {\n",
    "        \"description\": \"A distinctive biennial plant with large, fuzzy leaves that form a rosette in the first year and a tall flower spike in the second. It has a long history of medicinal use.\",\n",
    "        \"features\": [\"Large, very soft, fuzzy silver-green leaves\", \"Tall, thick flower stalk (up to 2 meters)\", \"Dense spike of yellow, five-petaled flowers\", \"Forms a basal rosette in its first year\"],\n",
    "        \"habitat\": \"Disturbed soil, pastures, roadsides, and sunny, open fields.\",\n",
    "        \"uses\": \"Traditional medicine (especially for respiratory ailments); leaves as tinder.\"\n",
    "    },\n",
    "    \"Red Clover\": {\n",
    "        \"description\": \"A common legume grown agriculturally for forage and soil health. Its globe-shaped, pinkish-purple flower heads are a familiar sight in meadows and lawns.\",\n",
    "        \"features\": [\"Three-leaflet leaves, often with a pale chevron mark\", \"Round, reddish-purple or pink flower head\", \"Low-growing, spreading habit\", \"Hairy stems and leaves\"],\n",
    "        \"habitat\": \"Meadows, lawns, fields, and roadsides.\",\n",
    "        \"uses\": \"Forage for livestock; nitrogen-fixing cover crop; traditional medicine; edible flowers.\"\n",
    "    },\n",
    "    \"Sunflower\": {\n",
    "        \"description\": \"A tall annual plant famous for its large flower head that tracks the sun. It is a major global agricultural crop for its seeds and oil.\",\n",
    "        \"features\": [\"Very large, daisy-like flower head\", \"Bright yellow outer petals (ray florets)\", \"Tall, thick, hairy stem\", \"Large, rough, heart-shaped leaves\"],\n",
    "        \"habitat\": \"Cultivated fields and gardens; native to prairies and dry areas.\",\n",
    "        \"uses\": \"Production of sunflower oil and edible seeds; ornamental plant; bird feed.\"\n",
    "    },\n",
    "    \"Tea Plant\": {\n",
    "        \"description\": \"An evergreen shrub (Camellia sinensis) whose leaves and buds are harvested and processed to produce tea, one of the world's most consumed beverages.\",\n",
    "        \"features\": [\"Glossy, dark green, serrated leaves\", \"White, fragrant flowers with yellow stamens\", \"Typically pruned to a waist-high shrub for cultivation\", \"Leathery leaf texture\"],\n",
    "        \"habitat\": \"Cultivated in tropical and subtropical regions with high rainfall and acidic soil.\",\n",
    "        \"uses\": \"Production of all types of tea (black, green, white, oolong); ornamental.\"\n",
    "    },\n",
    "    \"Wild Grape Vine\": {\n",
    "        \"description\": \"A climbing woody vine and the ancestor of most cultivated grapes. It uses tendrils to scale trees and produces small, tart fruit.\",\n",
    "        \"features\": [\"Woody, climbing vine\", \"Grasping tendrils opposite the leaves\", \"Large, lobed, heart-shaped leaves\", \"Clusters of small, dark, tart berries\"],\n",
    "        \"habitat\": \"Forests, riverbanks, fencerows, and woodland edges.\",\n",
    "        \"uses\": \"Wildlife food source; edible fruit (for jams and jellies); leaves used in cooking.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configured to use {MAX_IMAGES_PER_SPECIES} images per species\")\n",
    "print(f\"Include detailed responses: {INCLUDE_DETAILED_RESPONSES}\")\n",
    "print(f\"Train/Validation split: {int((1-VALIDATION_SPLIT)*100)}% train, {int(VALIDATION_SPLIT*100)}% validation\")\n",
    "print(f\"Use validation: {USE_VALIDATION}\")\n",
    "print(f\"Available species: {list(PLANT_DESCRIPTIONS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9CBpiISFa6C"
   },
   "source": [
    "Load the plant dataset with the custom dataset loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T18:37:21.485738Z",
     "iopub.status.busy": "2025-07-06T18:37:21.484812Z",
     "iopub.status.idle": "2025-07-06T18:38:30.395453Z",
     "shell.execute_reply": "2025-07-06T18:38:30.394558Z",
     "shell.execute_reply.started": "2025-07-06T18:37:21.485701Z"
    },
    "id": "reoBXmAn7HlN",
    "outputId": "15979499-2a1b-4524-e3ff-c2c052cfd544",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "class PlantDatasetLoader:\n",
    "    \"\"\"Custom plant dataset loader for species identification.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path: str):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "\n",
    "    def load_dataset(self, max_per_species: int = 200, validation_split: float = 0.2, seed: int = 42, split: str = \"train\") -> list:\n",
    "        \"\"\"Load dataset with specified number of images per species and train/test split.\"\"\"\n",
    "        random.seed(seed)\n",
    "        \n",
    "        all_sample_data = []\n",
    "        \n",
    "        # Process each species directory\n",
    "        for species_dir in self.dataset_path.iterdir():\n",
    "            if not species_dir.is_dir():\n",
    "                continue\n",
    "                \n",
    "            species_name = species_dir.name\n",
    "            print(f\"Processing species: {species_name}\")\n",
    "            \n",
    "            # Get all image files\n",
    "            image_files = []\n",
    "            for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
    "                image_files.extend(species_dir.glob(ext))\n",
    "            \n",
    "            print(f\"  Found {len(image_files)} images\")\n",
    "            \n",
    "            # Shuffle files for this species\n",
    "            random.shuffle(image_files)\n",
    "            \n",
    "            # Limit samples per species if specified\n",
    "            if max_per_species and len(image_files) > max_per_species:\n",
    "                image_files = image_files[:max_per_species]\n",
    "                print(f\"  Limited to {max_per_species} samples\")\n",
    "            \n",
    "            # Split this species's images into train/val\n",
    "            if validation_split > 0:\n",
    "                split_idx = int(len(image_files) * (1 - validation_split))\n",
    "                train_files = image_files[:split_idx]\n",
    "                val_files = image_files[split_idx:]\n",
    "                \n",
    "                # Choose which split to use\n",
    "                selected_files = train_files if split == \"train\" else val_files\n",
    "                print(f\"  Using {len(selected_files)} images for {split} split\")\n",
    "            else:\n",
    "                selected_files = image_files\n",
    "            \n",
    "            # Process selected images\n",
    "            for image_file in selected_files:\n",
    "                try:\n",
    "                    image = Image.open(image_file).convert(\"RGB\")\n",
    "                    all_sample_data.append({\n",
    "                        'image_path': str(image_file),\n",
    "                        'image': image,\n",
    "                        'species': species_name,\n",
    "                        'image_name': image_file.stem\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error loading {image_file}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        return all_sample_data\n",
    "\n",
    "# Load the plant dataset\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    loader = PlantDatasetLoader(DATASET_PATH)\n",
    "    \n",
    "    # Load training data\n",
    "    dataset_samples = loader.load_dataset(\n",
    "        max_per_species=MAX_IMAGES_PER_SPECIES, \n",
    "        validation_split=VALIDATION_SPLIT if USE_VALIDATION else 0,\n",
    "        split=\"train\"\n",
    "    )\n",
    "    print(f\"Loaded {len(dataset_samples)} training samples from plant dataset\")\n",
    "\n",
    "    # Show distribution\n",
    "    from collections import Counter\n",
    "    species_counts = Counter([sample['species'] for sample in dataset_samples])\n",
    "    print(f\"Training species distribution: {dict(species_counts)}\")\n",
    "    \n",
    "    # Optionally load validation data for evaluation\n",
    "    if USE_VALIDATION:\n",
    "        val_dataset_samples = loader.load_dataset(\n",
    "            max_per_species=MAX_IMAGES_PER_SPECIES,\n",
    "            validation_split=VALIDATION_SPLIT,\n",
    "            split=\"val\"\n",
    "        )\n",
    "        val_species_counts = Counter([sample['species'] for sample in val_dataset_samples])\n",
    "        print(f\"Loaded {len(val_dataset_samples)} validation samples\")\n",
    "        print(f\"Validation species distribution: {dict(val_species_counts)}\")\n",
    "    else:\n",
    "        val_dataset_samples = []\n",
    "        \n",
    "else:\n",
    "    print(f\"Dataset path not found: {DATASET_PATH}\")\n",
    "    print(\"Please update DATASET_PATH to point to your plant dataset location\")\n",
    "    dataset_samples = []\n",
    "    val_dataset_samples = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6i5Sx9In7vHi"
   },
   "source": [
    "Let's see how a sample from our plant dataset looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T18:38:30.622179Z",
     "iopub.status.busy": "2025-07-06T18:38:30.621441Z",
     "iopub.status.idle": "2025-07-06T18:38:30.923533Z",
     "shell.execute_reply": "2025-07-06T18:38:30.922735Z",
     "shell.execute_reply.started": "2025-07-06T18:38:30.622148Z"
    },
    "id": "dzE1OEXi7s3P",
    "outputId": "41a6e0b6-21cd-4eeb-a3f1-9d49f8a1940b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if dataset_samples:\n",
    "    print(\"Sample from plant dataset:\")\n",
    "    sample = dataset_samples[0]\n",
    "    print(f\"Image path: {sample['image_path']}\")\n",
    "    print(f\"Species: {sample['species']}\")\n",
    "    print(f\"Image name: {sample['image_name']}\")\n",
    "\n",
    "    # Display the image\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    img = Image.open(sample['image_path'])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Sample Image - Species: {sample['species']}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No dataset samples loaded. Please check the DATASET_PATH.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Datast + Dataset Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class DatasetCache:\n",
    "    \"\"\"Cache system for processed datasets to avoid expensive HF Dataset operations.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"./dataset_cache\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def _get_config_hash(self, config_dict: dict) -> str:\n",
    "        \"\"\"Generate hash from configuration to ensure cache validity.\"\"\"\n",
    "        # Sort keys to ensure consistent hashing\n",
    "        config_str = json.dumps(config_dict, sort_keys=True)\n",
    "        return hashlib.md5(config_str.encode()).hexdigest()\n",
    "    \n",
    "    def _get_cache_path(self, config_hash: str) -> Path:\n",
    "        \"\"\"Get cache directory path for given configuration.\"\"\"\n",
    "        return self.cache_dir / f\"dataset_{config_hash}\"\n",
    "    \n",
    "    def _get_metadata_path(self, config_hash: str) -> Path:\n",
    "        \"\"\"Get metadata file path for given configuration.\"\"\"\n",
    "        return self.cache_dir / f\"dataset_{config_hash}_metadata.json\"\n",
    "    \n",
    "    def get_cache_config(self, dataset_path, max_images_per_species, validation_split, \n",
    "                        include_detailed_responses, split=\"train\"):\n",
    "        \"\"\"Create configuration dict for cache validation.\"\"\"\n",
    "        return {\n",
    "            \"dataset_path\": str(dataset_path),\n",
    "            \"max_images_per_species\": max_images_per_species,\n",
    "            \"validation_split\": validation_split,\n",
    "            \"include_detailed_responses\": include_detailed_responses,\n",
    "            \"split\": split,\n",
    "            \"plant_descriptions\": str(PLANT_DESCRIPTIONS),\n",
    "            \"identification_prompts\": str(IDENTIFICATION_PROMPTS)\n",
    "        }\n",
    "    \n",
    "    def load_dataset(self, config_dict: dict) -> tuple:\n",
    "        \"\"\"Load cached dataset if available and valid.\"\"\"\n",
    "        config_hash = self._get_config_hash(config_dict)\n",
    "        cache_path = self._get_cache_path(config_hash)\n",
    "        metadata_path = self._get_metadata_path(config_hash)\n",
    "        \n",
    "        if cache_path.exists() and metadata_path.exists():\n",
    "            print(f\"üì¶ Loading processed dataset from cache: {cache_path}\")\n",
    "            try:\n",
    "                # Load metadata\n",
    "                with open(metadata_path, 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "                \n",
    "                # Load dataset using HF datasets\n",
    "                dataset = Dataset.load_from_disk(str(cache_path))\n",
    "                dataset_samples = metadata.get(\"dataset_samples\", [])\n",
    "                \n",
    "                print(f\"‚úÖ Loaded cached dataset with {len(dataset)} samples\")\n",
    "                return dataset, dataset_samples\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Cache load failed: {e}\")\n",
    "                return None, None\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    def save_dataset(self, config_dict: dict, dataset, dataset_samples: list = None):\n",
    "        \"\"\"Save processed dataset to cache.\"\"\"\n",
    "        config_hash = self._get_config_hash(config_dict)\n",
    "        cache_path = self._get_cache_path(config_hash)\n",
    "        metadata_path = self._get_metadata_path(config_hash)\n",
    "        \n",
    "        print(f\"üíæ Saving processed dataset to cache: {cache_path}\")\n",
    "        try:\n",
    "            # Save dataset using HF datasets\n",
    "            dataset.save_to_disk(str(cache_path))\n",
    "            \n",
    "            # Save metadata (without dataset_samples which can be large)\n",
    "            metadata = {\n",
    "                \"config\": config_dict,\n",
    "                \"timestamp\": str(pd.Timestamp.now()),\n",
    "                \"dataset_size\": len(dataset)\n",
    "            }\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "            \n",
    "            print(f\"‚úÖ Dataset cache saved successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Dataset cache save failed: {e}\")\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear all cached datasets.\"\"\"\n",
    "        import shutil\n",
    "        for cache_dir in self.cache_dir.glob(\"dataset_*\"):\n",
    "            if cache_dir.is_dir():\n",
    "                shutil.rmtree(cache_dir)\n",
    "        for metadata_file in self.cache_dir.glob(\"dataset_*_metadata.json\"):\n",
    "            metadata_file.unlink()\n",
    "        print(\"üóëÔ∏è  All dataset caches cleared\")\n",
    "    \n",
    "    def list_caches(self):\n",
    "        \"\"\"List all available caches with their configurations.\"\"\"\n",
    "        metadata_files = list(self.cache_dir.glob(\"dataset_*_metadata.json\"))\n",
    "        if not metadata_files:\n",
    "            print(\"No dataset caches found\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(metadata_files)} cached dataset sets:\")\n",
    "        for metadata_file in metadata_files:\n",
    "            try:\n",
    "                with open(metadata_file, 'r') as f:\n",
    "                    metadata = json.load(f)\n",
    "                config = metadata.get(\"config\", {})\n",
    "                timestamp = metadata.get(\"timestamp\", \"Unknown\")\n",
    "                dataset_size = metadata.get(\"dataset_size\", \"Unknown\")\n",
    "                \n",
    "                print(f\"  üìÅ {metadata_file.stem}\")\n",
    "                print(f\"     Dataset size: {dataset_size} samples\")\n",
    "                print(f\"     Max images per species: {config.get('max_images_per_species', 'N/A')}\")\n",
    "                print(f\"     Validation split: {config.get('validation_split', 'N/A')}\")\n",
    "                print(f\"     Include detailed: {config.get('include_detailed_responses', 'N/A')}\")\n",
    "                print(f\"     Split: {config.get('split', 'N/A')}\")\n",
    "                print(f\"     Timestamp: {timestamp}\")\n",
    "                print()\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error reading {metadata_file.name}: {e}\")\n",
    "\n",
    "# Initialize dataset cache system\n",
    "dataset_cache = DatasetCache()\n",
    "print(\"üöÄ Dataset cache system initialized (caches final processed datasets)\")\n",
    "print(\"üí° This will cache the expensive Dataset.from_dict() and dataset.map() operations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Xs0LXio7rfd"
   },
   "source": [
    "Now we'll convert our plant dataset samples into the conversation format for `Gemma-3`. Each sample will be formatted as a multimodal conversation with an image and question/answer about plant species identification. We remove the `<bos>` token using removeprefix(`'<bos>'`) since we're finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T18:38:51.146609Z",
     "iopub.status.busy": "2025-07-06T18:38:51.146070Z",
     "iopub.status.idle": "2025-07-06T18:49:16.676271Z",
     "shell.execute_reply": "2025-07-06T18:49:16.674762Z",
     "shell.execute_reply.started": "2025-07-06T18:38:51.146576Z"
    },
    "id": "1ahE8Ys37JDJ",
    "outputId": "f3301548-bc30-42bd-ef66-75a2cfff2e73",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_plant_conversations(dataset_samples):\n",
    "    \"\"\"Convert plant samples to conversation format.\"\"\"\n",
    "    conversations = []\n",
    "\n",
    "    for sample in dataset_samples:\n",
    "        species = sample['species']\n",
    "        image = sample['image']\n",
    "        \n",
    "        # Get species information\n",
    "        species_info = PLANT_DESCRIPTIONS.get(species, {})\n",
    "        description = species_info.get(\"description\", f\"A plant from the {species} species.\")\n",
    "        \n",
    "        # Basic identification conversation\n",
    "        prompt = random.choice(IDENTIFICATION_PROMPTS)\n",
    "        basic_response = f\"{species}.\"\n",
    "        \n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image\", \"image\": image}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": basic_response}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        conversations.append(conversation)\n",
    "        \n",
    "        # Add detailed feature conversation if enabled\n",
    "        if INCLUDE_DETAILED_RESPONSES and species_info:\n",
    "            features_text = \", \".join(species_info.get(\"features\", []))\n",
    "            habitat = species_info.get(\"habitat\", \"Various environments\")\n",
    "            uses = species_info.get(\"uses\", \"Various uses\")\n",
    "            \n",
    "            detailed_response = f\"This is {species}. Key identifying features include: {features_text}. Habitat: {habitat}. Uses: {uses}.\"\n",
    "            \n",
    "            detailed_conversation = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Can you describe the key features of this plant?\"},\n",
    "                        {\"type\": \"image\", \"image\": image}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": detailed_response}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "            conversations.append(detailed_conversation)\n",
    "            \n",
    "            # Usage conversation for edible/useful plants\n",
    "            if \"edible\" in uses.lower() or \"medicinal\" in uses.lower():\n",
    "                usage_conversation = [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": \"What can this plant be used for?\"},\n",
    "                            {\"type\": \"image\", \"image\": image}\n",
    "                        ]\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": f\"This is {species}. {uses}. Always ensure proper identification before consuming wild plants.\"}\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "                conversations.append(usage_conversation)\n",
    "\n",
    "    return conversations\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format conversations for training.\"\"\"\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False).removeprefix('<bos>') for convo in convos]\n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ CACHED DATASET CREATION - Use this instead of the expensive operations!\n",
    "# This version caches the final processed dataset to avoid re-running Dataset.from_dict() and dataset.map()\n",
    "\n",
    "if dataset_samples:\n",
    "    # Create cache configuration\n",
    "    cache_config = dataset_cache.get_cache_config(\n",
    "        dataset_path=DATASET_PATH,\n",
    "        max_images_per_species=MAX_IMAGES_PER_SPECIES,\n",
    "        validation_split=VALIDATION_SPLIT if USE_VALIDATION else 0,\n",
    "        include_detailed_responses=INCLUDE_DETAILED_RESPONSES,\n",
    "        split=\"train\"\n",
    "    )\n",
    "    \n",
    "    # Try to load from cache first\n",
    "    cached_dataset, cached_samples = dataset_cache.load_dataset(cache_config)\n",
    "    \n",
    "    if cached_dataset is not None:\n",
    "        print(\"‚úÖ Using cached dataset - skipping expensive Dataset.from_dict() and dataset.map()!\")\n",
    "        dataset = cached_dataset\n",
    "        if cached_samples:\n",
    "            dataset_samples = cached_samples\n",
    "    else:\n",
    "        print(\"‚è≥ Cache miss - creating dataset from scratch (this will take time)...\")\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Format conversations (this is relatively fast)\n",
    "        print(\"üìù Formatting conversations...\")\n",
    "        conversations = format_plant_conversations(dataset_samples)\n",
    "        \n",
    "        # Create HF Dataset (this is slow!)\n",
    "        print(\"üì¶ Creating HF Dataset from conversations...\")\n",
    "        dataset = Dataset.from_dict({\"conversations\": conversations})\n",
    "        \n",
    "        # Apply chat template (this is VERY slow!)\n",
    "        print(\"üîÑ Applying chat template to dataset...\")\n",
    "        dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"‚úÖ Dataset processing completed in {duration:.2f} seconds\")\n",
    "        \n",
    "        # Save to cache for next time\n",
    "        dataset_cache.save_dataset(cache_config, dataset, dataset_samples)\n",
    "\n",
    "    print(f\"‚úÖ Final dataset ready with {len(dataset)} samples\")\n",
    "    print(f\"Dataset columns: {dataset.column_names}\")\n",
    "else:\n",
    "    print(\"‚ùå No dataset samples to process. Please check the DATASET_PATH.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è CACHE MANAGEMENT UTILITIES\n",
    "# Use these functions to manage your dataset cache\n",
    "\n",
    "def show_cache_status():\n",
    "    \"\"\"Show current cache status and available caches.\"\"\"\n",
    "    print(\"üìä Dataset Cache Management System Status\")\n",
    "    print(\"=\" * 50)\n",
    "    dataset_cache.list_caches()\n",
    "\n",
    "def clear_all_caches():\n",
    "    \"\"\"Clear all cached datasets.\"\"\"\n",
    "    dataset_cache.clear_cache()\n",
    "    print(\"‚úÖ All dataset caches cleared!\")\n",
    "\n",
    "def get_cache_for_validation():\n",
    "    \"\"\"Get cached dataset for validation if available.\"\"\"\n",
    "    if not USE_VALIDATION:\n",
    "        print(\"‚ùå Validation is disabled. Set USE_VALIDATION = True to use validation caching.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Create cache configuration for validation\n",
    "    val_cache_config = dataset_cache.get_cache_config(\n",
    "        dataset_path=DATASET_PATH,\n",
    "        max_images_per_species=MAX_IMAGES_PER_SPECIES,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        include_detailed_responses=INCLUDE_DETAILED_RESPONSES,\n",
    "        split=\"val\"  # This is for validation split\n",
    "    )\n",
    "    \n",
    "    # Try to load validation dataset from cache\n",
    "    val_dataset, val_samples = dataset_cache.load_dataset(val_cache_config)\n",
    "    \n",
    "    if val_dataset is not None:\n",
    "        print(\"‚úÖ Found cached validation dataset!\")\n",
    "        return val_dataset, val_samples\n",
    "    else:\n",
    "        print(\"‚è≥ No validation cache found. You'll need to create validation dataset.\")\n",
    "        return None, None\n",
    "\n",
    "# Show current cache status\n",
    "show_cache_status()\n",
    "\n",
    "print(\"\\nüîß Available cache management functions:\")\n",
    "print(\"  ‚Ä¢ show_cache_status() - Show all available dataset caches\")  \n",
    "print(\"  ‚Ä¢ clear_all_caches() - Delete all cached datasets\")\n",
    "print(\"  ‚Ä¢ get_cache_for_validation() - Get cached validation dataset\")\n",
    "print(\"\\nüí° Benefits of dataset caching:\")\n",
    "print(\"  ‚Ä¢ Avoids expensive Dataset.from_dict() operation\")\n",
    "print(\"  ‚Ä¢ Skips time-consuming dataset.map() chat template application\")\n",
    "print(\"  ‚Ä¢ Cache automatically validates based on your configuration\")\n",
    "print(\"  ‚Ä¢ If you change settings, a new cache will be created automatically\")\n",
    "print(\"\\n‚ö° Performance improvement: ~10-100x faster on subsequent runs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndDUB23CGAC5"
   },
   "source": [
    "Let's see how the chat template formatted our mushroom conversation! Notice there is no `<bos>` token as the processor tokenizer will be adding one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-06T18:49:16.676678Z",
     "iopub.status.idle": "2025-07-06T18:49:16.676959Z",
     "shell.execute_reply": "2025-07-06T18:49:16.676823Z",
     "shell.execute_reply.started": "2025-07-06T18:49:16.676810Z"
    },
    "id": "gGFzmplrEy9I",
    "outputId": "30c1e2ac-332f-4b7e-d653-ab4aae946692",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if dataset_samples and len(dataset) > 0:\n",
    "    print(\"Formatted conversation example:\")\n",
    "    print(dataset[0][\"text\"])\n",
    "else:\n",
    "    print(\"No formatted dataset available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-06T18:49:16.677998Z",
     "iopub.status.idle": "2025-07-06T18:49:16.678308Z",
     "shell.execute_reply": "2025-07-06T18:49:16.678142Z",
     "shell.execute_reply.started": "2025-07-06T18:49:16.678127Z"
    },
    "id": "95_Nn-89DhsL",
    "outputId": "9a6feff2-8726-4533-fa2a-31bab11c8507",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Prepare validation dataset if available\n",
    "eval_dataset = None\n",
    "if USE_VALIDATION and val_dataset_samples:\n",
    "    # Convert validation samples to conversation format\n",
    "    val_conversations = format_plant_conversations(val_dataset_samples)\n",
    "    val_dataset = Dataset.from_dict({\"conversations\": val_conversations})\n",
    "    val_dataset = val_dataset.map(formatting_prompts_func, batched=True)\n",
    "    eval_dataset = val_dataset\n",
    "    print(f\"Prepared validation dataset with {len(eval_dataset)} samples\")\n",
    "\n",
    "# Configure training parameters based on dataset size\n",
    "if dataset_samples:\n",
    "    dataset_size = len(dataset)\n",
    "    # Adjust training steps based on dataset size\n",
    "    if dataset_size < 100:\n",
    "        max_steps = 20\n",
    "    elif dataset_size < 500:\n",
    "        max_steps = 40\n",
    "    else:\n",
    "        max_steps = 100\n",
    "\n",
    "    print(f\"Training dataset size: {dataset_size}\")\n",
    "    if eval_dataset:\n",
    "        print(f\"Validation dataset size: {len(eval_dataset)}\")\n",
    "    print(f\"Training steps: {max_steps}\")\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = dataset,\n",
    "        eval_dataset = eval_dataset, # Now includes validation data!\n",
    "        args = SFTConfig(\n",
    "            dataset_text_field = \"text\",\n",
    "            per_device_train_batch_size = 1,\n",
    "            gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "            warmup_steps = 5,\n",
    "            # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "            max_steps = max_steps,\n",
    "            learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "            logging_steps = 1,\n",
    "            optim = \"adamw_8bit\",\n",
    "            weight_decay = 0.01,\n",
    "            lr_scheduler_type = \"linear\",\n",
    "            seed = 3407,\n",
    "            report_to = \"none\", # Use this for WandB etc\n",
    "\n",
    "            eval_steps = 10,\n",
    "        ),\n",
    "    )\n",
    "else:\n",
    "    print(\"No dataset loaded. Cannot create trainer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_sGp5XlG6dq"
   },
   "source": [
    "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-06T18:49:16.679496Z",
     "iopub.status.idle": "2025-07-06T18:49:16.679744Z",
     "shell.execute_reply": "2025-07-06T18:49:16.679649Z",
     "shell.execute_reply.started": "2025-07-06T18:49:16.679639Z"
    },
    "id": "juQiExuBG5Bt",
    "outputId": "bf15a42d-d456-4b90-b09b-8ca2aecf32c0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if dataset_samples and 'trainer' in locals():\n",
    "    from unsloth.chat_templates import train_on_responses_only\n",
    "    trainer = train_on_responses_only(\n",
    "        trainer,\n",
    "        instruction_part = \"<start_of_turn>user\\n\",\n",
    "        response_part = \"<start_of_turn>model\\n\",\n",
    "    )\n",
    "    print(\"Trainer configured to train only on model responses.\")\n",
    "else:\n",
    "    print(\"Trainer not available for response masking configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv1NBUozV78l"
   },
   "source": [
    "Let's verify masking the instruction part is done! Let's print the 100th row again.  Notice how the sample only has a single `<bos>` as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-06T18:49:16.681242Z",
     "iopub.status.idle": "2025-07-06T18:49:16.681545Z",
     "shell.execute_reply": "2025-07-06T18:49:16.681383Z",
     "shell.execute_reply.started": "2025-07-06T18:49:16.681367Z"
    },
    "id": "LtsMVtlkUhja",
    "outputId": "627ed41f-4644-4ad9-efee-cbdaa295aa7f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if dataset_samples and 'trainer' in locals() and len(trainer.train_dataset) > 0:\n",
    "    print(\"Training dataset sample (input_ids):\")\n",
    "    print(tokenizer.decode(trainer.train_dataset[0][\"input_ids\"]))\n",
    "else:\n",
    "    print(\"Training dataset not available for inspection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Kyjy__m9KY3"
   },
   "source": [
    "Now let's print the masked out example - you should see only the answer is present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-06T18:49:16.682751Z",
     "iopub.status.idle": "2025-07-06T18:49:16.683510Z",
     "shell.execute_reply": "2025-07-06T18:49:16.683366Z",
     "shell.execute_reply.started": "2025-07-06T18:49:16.683337Z"
    },
    "id": "_rD6fl8EUxnG",
    "outputId": "1f244c63-4e53-4a67-dbb1-31f736d27f55",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if dataset_samples and 'trainer' in locals() and len(trainer.train_dataset) > 0:\n",
    "    print(\"Training dataset sample (labels - masked):\")\n",
    "    print(tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]]).replace(tokenizer.pad_token, \" \"))\n",
    "else:\n",
    "    print(\"Training dataset not available for label inspection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.status.busy": "2025-07-06T18:49:16.684538Z",
     "iopub.status.idle": "2025-07-06T18:49:16.684830Z",
     "shell.execute_reply": "2025-07-06T18:49:16.684724Z",
     "shell.execute_reply.started": "2025-07-06T18:49:16.684710Z"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "ddbd8b32-d4e5-4292-b12f-6117fd49c8ee",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNP1Uidk9mrz"
   },
   "source": [
    "# Let's train the model!\n",
    "\n",
    "To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqxqAZ7KJ4oL",
    "outputId": "880fdda2-3f50-49c9-b9e3-853075585d91"
   },
   "outputs": [],
   "source": [
    "if dataset_samples and 'trainer' in locals():\n",
    "    trainer_stats = trainer.train()\n",
    "    print(\"Training completed successfully!\")\n",
    "else:\n",
    "    print(\"Cannot start training: No dataset loaded or trainer not initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pCqnaKmlO1U9",
    "outputId": "f6b831c5-f947-43c7-dddd-f8a3a1435bc0"
   },
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "if 'trainer_stats' in locals():\n",
    "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "    used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "    lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "    print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "    print(\n",
    "        f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    "    )\n",
    "    print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "    print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "    print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "    print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "else:\n",
    "    print(\"Training was not executed, so no stats to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model via Unsloth native inference! According to the `Gemma-3` team, the recommended settings for inference are `temperature = 1.0, top_p = 0.95, top_k = 64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kR3gIAX-SM2q",
    "outputId": "840e3693-fe36-4b29-8808-52f529202321"
   },
   "outputs": [],
   "source": [
    "# Test the model on plant identification\n",
    "if dataset_samples:\n",
    "    # Use a sample from our dataset for testing\n",
    "    test_sample = dataset_samples[0]\n",
    "    test_image_path = test_sample['image_path']\n",
    "    test_image = Image.open(test_image_path)\n",
    "\n",
    "    print(f\"Testing with image: {test_image_path}\")\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What type of plant is this?\"},\n",
    "            {\"type\": \"image\", \"image\": test_image}\n",
    "        ]\n",
    "    }]\n",
    "\n",
    "    # Important: Use the original tokenizer_inference for multimodal inference\n",
    "    # The get_chat_template() function may break vision capabilities\n",
    "    inputs = tokenizer_inference.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "        tokenize = True,\n",
    "        return_dict = True,\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens = 64, # Longer response for plant descriptions\n",
    "        # Recommended Gemma-3 settings!\n",
    "        temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    )\n",
    "\n",
    "    print(f\"Testing on image: {test_sample['image_name']}\")\n",
    "    print(f\"Ground truth species: {test_sample['species']}\")\n",
    "    print(f\"Model response: {tokenizer_inference.batch_decode(outputs)[0]}\")\n",
    "else:\n",
    "    print(\"No dataset samples available for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrSvZObor0lY"
   },
   "source": [
    " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upcOlWe7A1vc",
    "outputId": "66cb7b7c-b312-422a-ba2f-71665d08cfc3"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gemma-3n-e2b-it-plant-8bit_lora\")  # Local saving\n",
    "tokenizer.save_pretrained(\"gemma-3n-e2b-it-plant-8bit_lora\")\n",
    "# model.push_to_hub(\"wrongryan/gemma-3n-it-plant-4bit\", token = \"XXX\") # Online saving\n",
    "# tokenizer.push_to_hub(\"wrongryan/gemma-3n-it-plant-4bit\", token = \"XXX\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKX_XKs_BNZR",
    "outputId": "24230e69-5394-48dd-d3b0-b4ffa6f644e3"
   },
   "outputs": [],
   "source": [
    "# if False:\n",
    "#     from unsloth import FastModel\n",
    "#     model, tokenizer = FastModel.from_pretrained(\n",
    "#         model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "#         max_seq_length = 2048,\n",
    "#         load_in_4bit = True,\n",
    "#     )\n",
    "\n",
    "# messages = [{\n",
    "#     \"role\": \"user\",\n",
    "#     \"content\": [{\"type\" : \"text\", \"text\" : \"What is Gemma-3N?\",}]\n",
    "# }]\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     return_tensors = \"pt\",\n",
    "#     tokenize = True,\n",
    "#     return_dict = True,\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "# from transformers import TextStreamer\n",
    "# _ = model.generate(\n",
    "#     **inputs,\n",
    "#     max_new_tokens = 128, # Increase for longer outputs!\n",
    "#     # Recommended Gemma-3 settings!\n",
    "#     temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "#     streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly for deployment! We save it in the folder `gemma-3N-finetune`. Set `if False` to `if True` to let it run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHjt_SMYsd3P",
    "outputId": "4f7a369f-d734-4d54-fb60-65b63c43ab09"
   },
   "outputs": [],
   "source": [
    "if True: # Change to True to save finetune!\n",
    "    model.save_pretrained_merged(\"gemma-3n-e2b-it-plant-8bit_lora\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6O48DbNIAr0"
   },
   "source": [
    "If you want to upload / push to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV-CiKPrIFG0"
   },
   "outputs": [],
   "source": [
    "if True: # Change to True to upload finetune\n",
    "    model.push_to_hub_merged(\n",
    "        \"wrongryan/gemma-3n-e2b-it-plant-8bit-lora\", tokenizer,\n",
    "        token = \"XXX\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Test Set Evaluation (Valid Species Only)\n",
    "\n",
    "Let's evaluate the model's performance on a proper test set with comprehensive metrics and analysis, filtering to only include valid species from our training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get valid species from training data\n",
    "def get_valid_species_from_training():\n",
    "    return list(PLANT_DESCRIPTIONS.keys())\n",
    "\n",
    "# Load test set and filter for valid species only\n",
    "TEST_DATASET_PATH = \"../../data/plants/test/\"  # Path to the holdout test set\n",
    "\n",
    "print(\"Loading test set...\")\n",
    "if os.path.exists(TEST_DATASET_PATH):\n",
    "    test_loader = PlantDatasetLoader(TEST_DATASET_PATH)\n",
    "    all_test_samples = test_loader.load_dataset(\n",
    "        max_per_species=None,  # Use all available test images\n",
    "        validation_split=0,    # No split needed, use all as test\n",
    "        split=\"train\"         # Using \"train\" parameter but loading from test directory\n",
    "    )\n",
    "    print(f\"Loaded {len(all_test_samples)} total test samples\")\n",
    "    \n",
    "    # Get valid species from training\n",
    "    valid_species = get_valid_species_from_training()\n",
    "    print(f\"Valid species from training: {len(valid_species)} species\")\n",
    "    print(f\"Valid species: {valid_species[:10]}{'...' if len(valid_species) > 10 else ''}\")\n",
    "    \n",
    "    # Filter test samples to only include valid species\n",
    "    test_samples = [sample for sample in all_test_samples if sample['species'] in valid_species]\n",
    "    \n",
    "    print(f\"Filtered to {len(test_samples)} test samples with valid species\")\n",
    "    print(f\"Excluded {len(all_test_samples) - len(test_samples)} samples with invalid species\")\n",
    "    \n",
    "    # Show test set distribution for valid species\n",
    "    from collections import Counter\n",
    "    test_species_counts = Counter([sample['species'] for sample in test_samples])\n",
    "    invalid_species = Counter([sample['species'] for sample in all_test_samples if sample['species'] not in valid_species])\n",
    "    \n",
    "    print(f\"\\\\nValid species in test set: {len(test_species_counts)} species\")\n",
    "    print(f\"Test set species distribution (top 10): {dict(list(test_species_counts.most_common(10)))}\")\n",
    "    \n",
    "    if invalid_species:\n",
    "        print(f\"\\\\nExcluded species: {len(invalid_species)} species\")\n",
    "        print(f\"Excluded species (top 5): {dict(list(invalid_species.most_common(5)))}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"Test dataset path not found: {TEST_DATASET_PATH}\")\n",
    "    print(\"Please ensure you have a test directory with the holdout test set\")\n",
    "    test_samples = []\n",
    "    valid_species = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def extract_species_from_response(response_text, valid_species):\n",
    "    \"\"\"Extract the predicted species from model response text.\"\"\"\n",
    "    # Convert response to lowercase for matching\n",
    "    response_lower = response_text.lower()\n",
    "    \n",
    "    # Create normalized species names for matching\n",
    "    normalized_species = {}\n",
    "    for species in valid_species:\n",
    "        # Store original -> normalized mapping\n",
    "        normalized_species[species.lower().replace(' ', '').replace('-', '')] = species\n",
    "        normalized_species[species.lower()] = species\n",
    "    \n",
    "    # Common patterns to look for species mentions\n",
    "    patterns = [\n",
    "        # Direct patterns like \"This is a Dandelion plant\"\n",
    "        r'(?:this|it|these)\\\\s+(?:is|are|appears?|looks?|seems?)\\\\s+(?:a|an)?\\\\s*([\\\\w\\\\s]+?)\\\\s+(?:plant|species|flower|weed)',\n",
    "        # Patterns like \"Dandelion species\" or \"Dandelion plant\"\n",
    "        r'([\\\\w\\\\s]+?)\\\\s+(?:species|plant|flower|weed)',\n",
    "        # Patterns like \"species Dandelion\"\n",
    "        r'species\\\\s+([\\\\w\\\\s]+)',\n",
    "        # Patterns with **bold** markdown\n",
    "        r'\\\\*\\\\*([\\\\w\\\\s]+?)\\\\*\\\\*',\n",
    "        # Direct species names at start of sentences\n",
    "        r'(?:^|\\\\.\\\\s+)([\\\\w\\\\s]+?)\\\\s+(?:is|are|species)',\n",
    "        # Common plant-specific patterns\n",
    "        r'(?:plant|flower|weed|species).*?([\\\\w\\\\s]+)',\n",
    "        # Just capture potential species names (2-3 words)\n",
    "        r'\\\\b([A-Z][a-z]+(?:\\\\s+[A-Z][a-z]+){0,2})\\\\b',\n",
    "    ]\n",
    "    \n",
    "    # First try exact species name matching\n",
    "    for species in valid_species:\n",
    "        if species.lower() in response_lower:\n",
    "            return species\n",
    "    \n",
    "    # Then try pattern matching\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, response_lower, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            match = match.strip()\n",
    "            if not match:\n",
    "                continue\n",
    "                \n",
    "            # Try direct match\n",
    "            if match.lower() in normalized_species:\n",
    "                return normalized_species[match.lower()]\n",
    "            \n",
    "            # Try normalized match (remove spaces/hyphens)\n",
    "            normalized_match = match.lower().replace(' ', '').replace('-', '')\n",
    "            if normalized_match in normalized_species:\n",
    "                return normalized_species[normalized_match]\n",
    "            \n",
    "            # Try partial matching for compound names\n",
    "            for species in valid_species:\n",
    "                species_words = species.lower().split()\n",
    "                match_words = match.lower().split()\n",
    "                \n",
    "                # If all words in match are in species name\n",
    "                if len(match_words) >= 2 and all(word in species_words for word in match_words):\n",
    "                    return species\n",
    "                    \n",
    "                # If match contains the key part of species name\n",
    "                if len(species_words) >= 2 and any(word in match.lower() for word in species_words):\n",
    "                    if len([word for word in species_words if word in match.lower()]) >= len(species_words) // 2:\n",
    "                        return species\n",
    "    \n",
    "    return \"Unknown\"\n",
    "\n",
    "def run_test_evaluation_valid_species(test_samples, model, tokenizer_inference, valid_species, max_samples=None):\n",
    "    \"\"\"Run comprehensive evaluation on test samples with valid species only.\"\"\"\n",
    "    if not test_samples:\n",
    "        print(\"No test samples available for evaluation.\")\n",
    "        return [], [], []\n",
    "    \n",
    "    # Limit test samples if specified\n",
    "    if max_samples and len(test_samples) > max_samples:\n",
    "        test_samples = random.sample(test_samples, max_samples)\n",
    "        print(f\"Limited test set to {max_samples} samples for faster evaluation\")\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    responses = []\n",
    "    \n",
    "    print(f\"Running inference on {len(test_samples)} test samples for {len(valid_species)} valid species...\")\n",
    "    \n",
    "    # Run inference on each test sample\n",
    "    for i, sample in enumerate(tqdm(test_samples, desc=\"Testing Plants\")):\n",
    "        try:\n",
    "            # Prepare the message - using plant-specific prompt\n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"What type of plant is this? Please respond concisely.\"},\n",
    "                    {\"type\": \"image\", \"image\": sample['image']}\n",
    "                ]\n",
    "            }]\n",
    "            \n",
    "            # Prepare inputs\n",
    "            inputs = tokenizer_inference.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\",\n",
    "                tokenize=True,\n",
    "                return_dict=True,\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,  # Keep responses concise\n",
    "                    temperature=1.0, \n",
    "                    top_p=0.95, \n",
    "                    top_k=64,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer_inference.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            full_response = tokenizer_inference.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "            \n",
    "            # Extract just the model's response (after the user message)\n",
    "            response_parts = full_response.split(\"<start_of_turn>model\")\n",
    "            if len(response_parts) > 1:\n",
    "                model_response = response_parts[-1].replace(\"<end_of_turn>\", \"\").strip()\n",
    "            else:\n",
    "                model_response = full_response\n",
    "            \n",
    "            # Extract species prediction\n",
    "            predicted_species = extract_species_from_response(model_response, valid_species)\n",
    "            \n",
    "            # Store results\n",
    "            predictions.append(predicted_species)\n",
    "            ground_truths.append(sample['species'])\n",
    "            responses.append(model_response)\n",
    "            \n",
    "            # Show progress for first few samples\n",
    "            if i < 5:\n",
    "                print(f\"\\\\nSample {i+1}:\")\n",
    "                print(f\"  Ground truth: {sample['species']}\")\n",
    "                print(f\"  Model response: {model_response[:100]}...\")\n",
    "                print(f\"  Predicted species: {predicted_species}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {i}: {e}\")\n",
    "            predictions.append(\"Error\")\n",
    "            ground_truths.append(sample['species'])\n",
    "            responses.append(f\"Error: {e}\")\n",
    "    \n",
    "    return predictions, ground_truths, responses\n",
    "\n",
    "# Run the evaluation on valid species only\n",
    "if test_samples and valid_species:\n",
    "    predictions, ground_truths, responses = run_test_evaluation_valid_species(\n",
    "        test_samples, model, tokenizer_inference, valid_species, max_samples=2000\n",
    "    )\n",
    "    print(f\"\\\\nCompleted evaluation on {len(predictions)} samples for valid species\")\n",
    "    \n",
    "    # Show prediction accuracy for valid vs invalid species\n",
    "    valid_predictions = [pred for pred, truth in zip(predictions, ground_truths) if pred in valid_species]\n",
    "    invalid_predictions = [pred for pred, truth in zip(predictions, ground_truths) if pred not in valid_species and pred != \"Error\"]\n",
    "    \n",
    "    print(f\"Predictions mapping to valid species: {len(valid_predictions)}/{len(predictions)} ({100*len(valid_predictions)/len(predictions):.1f}%)\")\n",
    "    print(f\"Predictions mapping to invalid species: {len(invalid_predictions)}\")\n",
    "    print(f\"Error predictions: {predictions.count('Error')}\")\n",
    "    \n",
    "else:\n",
    "    predictions, ground_truths, responses = [], [], []\n",
    "    print(\"Skipping evaluation - no test samples or valid species loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive classification metrics for valid species only\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def calculate_classification_metrics_valid_species(predictions, ground_truths, valid_species):\n",
    "    \"\"\"Calculate comprehensive classification metrics for valid species only.\"\"\"\n",
    "    if not predictions or not ground_truths:\n",
    "        print(\"No predictions or ground truths available for metrics calculation.\")\n",
    "        return {}\n",
    "    \n",
    "    # Filter to include only:\n",
    "    # 1. Valid predictions (not \"Error\" or \"Unknown\")\n",
    "    # 2. Ground truth species that are in valid_species \n",
    "    # 3. Predictions that map to valid species\n",
    "    valid_indices = []\n",
    "    for i, (pred, truth) in enumerate(zip(predictions, ground_truths)):\n",
    "        if (pred != \"Error\" and pred != \"Unknown\" and \n",
    "            truth in valid_species and pred in valid_species):\n",
    "            valid_indices.append(i)\n",
    "    \n",
    "    valid_predictions = [predictions[i] for i in valid_indices]\n",
    "    valid_ground_truths = [ground_truths[i] for i in valid_indices]\n",
    "    \n",
    "    if not valid_predictions:\n",
    "        print(\"No valid predictions available for metrics calculation.\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Calculating metrics for {len(valid_predictions)} valid predictions from {len(valid_species)} species\")\n",
    "    \n",
    "    # Overall accuracy (only for valid species)\n",
    "    accuracy = accuracy_score(valid_ground_truths, valid_predictions)\n",
    "    \n",
    "    # Get unique labels that appear in both predictions and ground truth\n",
    "    all_labels = sorted(list(set(valid_ground_truths + valid_predictions)))\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        valid_ground_truths, valid_predictions, labels=all_labels, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Macro and weighted averages\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        valid_ground_truths, valid_predictions, average='macro', zero_division=0\n",
    "    )\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        valid_ground_truths, valid_predictions, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(valid_ground_truths, valid_predictions, labels=all_labels)\n",
    "    \n",
    "    # Calculate coverage metrics\n",
    "    predicted_species = set([pred for pred in predictions if pred in valid_species])\n",
    "    ground_truth_species = set([truth for truth in ground_truths if truth in valid_species])\n",
    "    \n",
    "    species_coverage = len(predicted_species) / len(valid_species) if valid_species else 0\n",
    "    species_recall = len(predicted_species.intersection(ground_truth_species)) / len(ground_truth_species) if ground_truth_species else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'per_class_precision': dict(zip(all_labels, precision)),\n",
    "        'per_class_recall': dict(zip(all_labels, recall)),\n",
    "        'per_class_f1': dict(zip(all_labels, f1)),\n",
    "        'per_class_support': dict(zip(all_labels, support)),\n",
    "        'confusion_matrix': cm,\n",
    "        'labels': all_labels,\n",
    "        'valid_predictions': valid_predictions,\n",
    "        'valid_ground_truths': valid_ground_truths,\n",
    "        'total_samples': len(predictions),\n",
    "        'valid_samples': len(valid_predictions),\n",
    "        'error_samples': predictions.count(\"Error\"),\n",
    "        'unknown_samples': predictions.count(\"Unknown\"),\n",
    "        'invalid_species_predictions': len([p for p in predictions if p not in valid_species and p not in [\"Error\", \"Unknown\"]]),\n",
    "        'species_coverage': species_coverage,\n",
    "        'species_recall': species_recall,\n",
    "        'total_valid_species': len(valid_species),\n",
    "        'predicted_species_count': len(predicted_species),\n",
    "        'ground_truth_species_count': len(ground_truth_species)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics for valid species only\n",
    "if predictions and ground_truths and valid_species:\n",
    "    metrics = calculate_classification_metrics_valid_species(predictions, ground_truths, valid_species)\n",
    "    \n",
    "    if metrics:\n",
    "        print(\"\\\\n\" + \"=\"*60)\n",
    "        print(\"VALID SPECIES CLASSIFICATION METRICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Overall statistics\n",
    "        print(f\"Total test samples: {metrics['total_samples']}\")\n",
    "        print(f\"Valid species predictions: {metrics['valid_samples']}\")\n",
    "        print(f\"Error samples: {metrics['error_samples']}\")\n",
    "        print(f\"Unknown species samples: {metrics['unknown_samples']}\")\n",
    "        print(f\"Invalid species predictions: {metrics['invalid_species_predictions']}\")\n",
    "        \n",
    "        # Species coverage analysis\n",
    "        print(f\"\\\\nSpecies Coverage Analysis:\")\n",
    "        print(f\"Total valid species in training: {metrics['total_valid_species']}\")\n",
    "        print(f\"Species represented in test set: {metrics['ground_truth_species_count']}\")\n",
    "        print(f\"Species predicted by model: {metrics['predicted_species_count']}\")\n",
    "        print(f\"Species coverage: {metrics['species_coverage']:.3f} ({metrics['predicted_species_count']}/{metrics['total_valid_species']})\")\n",
    "        print(f\"Species recall: {metrics['species_recall']:.3f}\")\n",
    "        \n",
    "        # Classification performance\n",
    "        print(f\"\\\\nClassification Performance (Valid Species Only):\")\n",
    "        print(f\"Overall Accuracy: {metrics['accuracy']:.3f}\")\n",
    "        print(f\"Macro-averaged Precision: {metrics['precision_macro']:.3f}\")\n",
    "        print(f\"Macro-averaged Recall: {metrics['recall_macro']:.3f}\")\n",
    "        print(f\"Macro-averaged F1-score: {metrics['f1_macro']:.3f}\")\n",
    "        print(f\"Weighted-averaged Precision: {metrics['precision_weighted']:.3f}\")\n",
    "        print(f\"Weighted-averaged Recall: {metrics['recall_weighted']:.3f}\")\n",
    "        print(f\"Weighted-averaged F1-score: {metrics['f1_weighted']:.3f}\")\n",
    "        \n",
    "        # Top performing species\n",
    "        print(f\"\\\\nTop Performing Species (F1-Score):\")\n",
    "        print(\"-\"*50)\n",
    "        species_f1 = [(species, f1) for species, f1 in metrics['per_class_f1'].items()]\n",
    "        species_f1.sort(key=lambda x: x[1], reverse=True)\n",
    "        for species, f1 in species_f1[:10]:\n",
    "            precision = metrics['per_class_precision'][species]\n",
    "            recall = metrics['per_class_recall'][species]\n",
    "            support = metrics['per_class_support'][species]\n",
    "            print(f\"{species:25} | F1: {f1:.3f} | P: {precision:.3f} | R: {recall:.3f} | Support: {support}\")\n",
    "        \n",
    "        # Detailed classification report\n",
    "        print(f\"\\\\nDetailed Classification Report (Valid Species):\")\n",
    "        print(\"-\"*60)\n",
    "        print(classification_report(metrics['valid_ground_truths'], metrics['valid_predictions']))\n",
    "        \n",
    "else:\n",
    "    print(\"No predictions available for metrics calculation.\")\n",
    "    metrics = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations and analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def create_evaluation_visualizations(metrics):\n",
    "    \"\"\"Create comprehensive evaluation visualizations.\"\"\"\n",
    "    if not metrics:\n",
    "        print(\"No metrics available for visualization.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    cm = metrics['confusion_matrix']\n",
    "    labels = metrics['labels']\n",
    "    \n",
    "    # Create a custom colormap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                cbar_kws={'label': 'Number of Samples'})\n",
    "    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Predicted Species', fontsize=12)\n",
    "    plt.ylabel('True Species', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # 2. Per-Class Accuracy Bar Chart\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    per_class_acc = []\n",
    "    class_names = []\n",
    "    for label in labels:\n",
    "        if label in metrics['per_class_recall']:\n",
    "            per_class_acc.append(metrics['per_class_recall'][label])\n",
    "            class_names.append(label)\n",
    "    \n",
    "    bars = plt.bar(range(len(class_names)), per_class_acc, \n",
    "                   color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22'])\n",
    "    plt.title('Per-Species Recall (Sensitivity)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Species', fontsize=12)\n",
    "    plt.ylabel('Recall', fontsize=12)\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "    plt.ylim(0, 1.1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, acc) in enumerate(zip(bars, per_class_acc)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Precision vs Recall Scatter Plot\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    precisions = [metrics['per_class_precision'][label] for label in labels if label in metrics['per_class_precision']]\n",
    "    recalls = [metrics['per_class_recall'][label] for label in labels if label in metrics['per_class_recall']]\n",
    "    \n",
    "    scatter = plt.scatter(recalls, precisions, s=100, alpha=0.7, \n",
    "                         c=range(len(labels)), cmap='tab10')\n",
    "    plt.title('Precision vs Recall by Species', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.xlim(0, 1.1)\n",
    "    plt.ylim(0, 1.1)\n",
    "    \n",
    "    # Add diagonal line (perfect precision = recall)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect P=R')\n",
    "    \n",
    "    # Add species labels\n",
    "    for i, label in enumerate(class_names):\n",
    "        if i < len(recalls) and i < len(precisions):\n",
    "            plt.annotate(label, (recalls[i], precisions[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. F1-Score Bar Chart\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    f1_scores = [metrics['per_class_f1'][label] for label in labels if label in metrics['per_class_f1']]\n",
    "    \n",
    "    bars = plt.bar(range(len(class_names)), f1_scores, \n",
    "                   color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22'])\n",
    "    plt.title('Per-Species F1-Score', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Species', fontsize=12)\n",
    "    plt.ylabel('F1-Score', fontsize=12)\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "    plt.ylim(0, 1.1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, f1) in enumerate(zip(bars, f1_scores)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{f1:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 5. Sample Support Distribution\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    supports = [metrics['per_class_support'][label] for label in labels if label in metrics['per_class_support']]\n",
    "    \n",
    "    bars = plt.bar(range(len(class_names)), supports, \n",
    "                   color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc', '#c2c2f0', '#ffb3e6', '#c4e17f', '#ffff99'])\n",
    "    plt.title('Test Set Sample Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Species', fontsize=12)\n",
    "    plt.ylabel('Number of Test Samples', fontsize=12)\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, support) in enumerate(zip(bars, supports)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                f'{support}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 6. Overall Metrics Summary\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Create a summary text\n",
    "    summary_text = f\"\"\"\n",
    "    OVERALL PERFORMANCE SUMMARY\n",
    "    \n",
    "    Total Test Samples: {metrics['total_samples']}\n",
    "    Valid Predictions: {metrics['valid_samples']}\n",
    "    Error Rate: {metrics['error_samples']}/{metrics['total_samples']} ({100*metrics['error_samples']/metrics['total_samples']:.1f}%)\n",
    "    \n",
    "    Overall Accuracy: {metrics['accuracy']:.3f}\n",
    "    \n",
    "    Macro Averages:\n",
    "    ‚Ä¢ Precision: {metrics['precision_macro']:.3f}\n",
    "    ‚Ä¢ Recall: {metrics['recall_macro']:.3f}\n",
    "    ‚Ä¢ F1-Score: {metrics['f1_macro']:.3f}\n",
    "    \n",
    "    Weighted Averages:\n",
    "    ‚Ä¢ Precision: {metrics['precision_weighted']:.3f}\n",
    "    ‚Ä¢ Recall: {metrics['recall_weighted']:.3f}\n",
    "    ‚Ä¢ F1-Score: {metrics['f1_weighted']:.3f}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax6.text(0.1, 0.9, summary_text, transform=ax6.transAxes, fontsize=11,\n",
    "             verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional Analysis: Top Confusion Pairs\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CONFUSION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Find most confused pairs\n",
    "    confusion_pairs = []\n",
    "    for i, true_label in enumerate(labels):\n",
    "        for j, pred_label in enumerate(labels):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                confusion_pairs.append((true_label, pred_label, cm[i, j]))\n",
    "    \n",
    "    # Sort by confusion count\n",
    "    confusion_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print(\"Most Common Misclassifications:\")\n",
    "    for true_label, pred_label, count in confusion_pairs[:10]:\n",
    "        print(f\"  {true_label} ‚Üí {pred_label}: {count} samples\")\n",
    "    \n",
    "    if not confusion_pairs:\n",
    "        print(\"  No misclassifications found! Perfect performance!\")\n",
    "\n",
    "# Create visualizations\n",
    "if metrics:\n",
    "    create_evaluation_visualizations(metrics)\n",
    "else:\n",
    "    print(\"No metrics available for visualization creation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation summary and completion status\n",
    "if test_samples and predictions and ground_truths and valid_species:\n",
    "    print(\"\\\\n\" + \"=\"*70)\n",
    "    print(\"PLANT SPECIES TEST SET EVALUATION COMPLETED!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"‚úÖ Loaded test set and filtered for valid species\")\n",
    "    print(\"‚úÖ Ran inference on test samples\") \n",
    "    print(\"‚úÖ Calculated classification metrics for valid species only\")\n",
    "    print(\"‚úÖ Generated comprehensive visualizations and analysis\")\n",
    "    print(\"‚úÖ Analyzed species coverage and performance\")\n",
    "    \n",
    "    print(f\"\\\\nKey Results:\")\n",
    "    print(f\"- Evaluated {len(test_samples)} test samples\")\n",
    "    print(f\"- Valid species predictions: {metrics['valid_samples']} ({100*metrics['valid_samples']/len(predictions):.1f}%)\")\n",
    "    print(f\"- Overall accuracy on valid species: {metrics['accuracy']:.3f}\")\n",
    "    print(f\"- Species coverage: {metrics['predicted_species_count']}/{metrics['total_valid_species']} ({metrics['species_coverage']:.1%})\")\n",
    "    \n",
    "    # Show some example correct and incorrect predictions\n",
    "    correct_examples = [(pred, truth) for pred, truth in zip(predictions, ground_truths) if pred == truth and pred in valid_species]\n",
    "    incorrect_examples = [(pred, truth) for pred, truth in zip(predictions, ground_truths) if pred != truth and pred in valid_species and truth in valid_species]\n",
    "    \n",
    "    if correct_examples:\n",
    "        print(f\"\\\\nExample correct predictions: {correct_examples[:3]}\")\n",
    "    if incorrect_examples:\n",
    "        print(f\"Example incorrect predictions: {incorrect_examples[:3]}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Cannot complete evaluation - missing test data, predictions, or valid species.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example predictions for qualitative analysis\n",
    "def show_prediction_examples(test_samples, predictions, ground_truths, responses, num_examples=8):\n",
    "    \"\"\"Show example predictions with images for qualitative analysis.\"\"\"\n",
    "    if not test_samples or not predictions:\n",
    "        print(\"No test samples or predictions available for examples.\")\n",
    "        return\n",
    "    \n",
    "    # Select examples: some correct, some incorrect\n",
    "    correct_indices = [i for i, (pred, true) in enumerate(zip(predictions, ground_truths)) \n",
    "                      if pred == true and pred != \"Error\"]\n",
    "    incorrect_indices = [i for i, (pred, true) in enumerate(zip(predictions, ground_truths)) \n",
    "                        if pred != true and pred != \"Error\"]\n",
    "    \n",
    "    # Select examples to show\n",
    "    selected_indices = []\n",
    "    \n",
    "    # Add some correct examples\n",
    "    if correct_indices:\n",
    "        selected_indices.extend(random.sample(correct_indices, min(4, len(correct_indices))))\n",
    "    \n",
    "    # Add some incorrect examples  \n",
    "    if incorrect_indices:\n",
    "        selected_indices.extend(random.sample(incorrect_indices, min(4, len(incorrect_indices))))\n",
    "    \n",
    "    # If we don't have enough, fill with any available\n",
    "    if len(selected_indices) < num_examples:\n",
    "        remaining_indices = [i for i in range(len(predictions)) if i not in selected_indices and predictions[i] != \"Error\"]\n",
    "        selected_indices.extend(random.sample(remaining_indices, min(num_examples - len(selected_indices), len(remaining_indices))))\n",
    "    \n",
    "    if not selected_indices:\n",
    "        print(\"No valid examples to display.\")\n",
    "        return\n",
    "    \n",
    "    # Create the visualization\n",
    "    cols = 4\n",
    "    rows = (len(selected_indices) + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 5*rows))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, example_idx in enumerate(selected_indices):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        ax = axes[row, col] if rows > 1 else axes[col]\n",
    "        \n",
    "        # Get example data\n",
    "        sample = test_samples[example_idx]\n",
    "        pred = predictions[example_idx]\n",
    "        true = ground_truths[example_idx]\n",
    "        response = responses[example_idx]\n",
    "        \n",
    "        # Display image\n",
    "        ax.imshow(sample['image'])\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Create title with prediction info\n",
    "        is_correct = pred == true\n",
    "        status = \"‚úì CORRECT\" if is_correct else \"‚úó INCORRECT\"\n",
    "        color = 'green' if is_correct else 'red'\n",
    "        \n",
    "        title = f\"{status}\\nTrue: {true}\\nPred: {pred}\"\n",
    "        ax.set_title(title, fontsize=10, fontweight='bold', color=color, pad=10)\n",
    "        \n",
    "        # Add response as text below\n",
    "        response_text = response[:60] + \"...\" if len(response) > 60 else response\n",
    "        ax.text(0.5, -0.1, f\"Response: {response_text}\", \n",
    "                transform=ax.transAxes, ha='center', va='top', \n",
    "                fontsize=8, wrap=True, style='italic')\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for idx in range(len(selected_indices), rows * cols):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        ax = axes[row, col] if rows > 1 else axes[col]\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXAMPLE PREDICTIONS ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    correct_count = len(correct_indices)\n",
    "    incorrect_count = len(incorrect_indices)\n",
    "    total_valid = correct_count + incorrect_count\n",
    "    \n",
    "    print(f\"Showing {len(selected_indices)} example predictions:\")\n",
    "    print(f\"  ‚Ä¢ Correct predictions available: {correct_count}/{total_valid}\")\n",
    "    print(f\"  ‚Ä¢ Incorrect predictions available: {incorrect_count}/{total_valid}\")\n",
    "    \n",
    "    if incorrect_indices:\n",
    "        print(f\"\\nSample of incorrect predictions:\")\n",
    "        for i, idx in enumerate(incorrect_indices[:5]):\n",
    "            pred = predictions[idx]\n",
    "            true = ground_truths[idx]\n",
    "            print(f\"  {i+1}. True: {true} ‚Üí Predicted: {pred}\")\n",
    "\n",
    "# Show example predictions\n",
    "if test_samples and predictions and ground_truths:\n",
    "    show_prediction_examples(test_samples, predictions, ground_truths, responses)\n",
    "    \n",
    "    # Update todo status\n",
    "    print(\"\\nüéâ Test set evaluation completed!\")\n",
    "    print(\"‚úÖ Loaded test set\")\n",
    "    print(\"‚úÖ Ran inference on test samples\") \n",
    "    print(\"‚úÖ Calculated classification metrics\")\n",
    "    print(\"‚úÖ Generated comprehensive visualizations\")\n",
    "    print(\"‚úÖ Analyzed prediction examples\")\n",
    "else:\n",
    "    print(\"Cannot show examples - missing test data or predictions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now for all models! For now, you can convert easily to `Q8_0, F16 or BF16` precision. `Q4_K_M` for 4bit will come later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "if True: # Change to True to save to GGUF\n",
    "    model.save_pretrained_gguf(\n",
    "        \"gemma-3n-it-plant-4bit_gguf\",\n",
    "        quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q974YEVPI7JS"
   },
   "source": [
    "Likewise, if you want to instead push to GGUF to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgcJIhJ0I_es"
   },
   "outputs": [],
   "source": [
    "if True: # Change to True to upload GGUF\n",
    "    model.push_to_hub_gguf(\n",
    "        \"gemma-3n-it-plant-4bit\",\n",
    "        quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
    "        repo_id = \"wrongryan/gemma-3n-it-plant-4bit-gguf\",\n",
    "        token = \"XXX\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4574621,
     "sourceId": 7810505,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0380ac8639cc4f5d9fb8949a9a3a62b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0771e36286134f6190d6cc28099059ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0912ed2603ae486594d8dedb926ad515": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09829dddead146c89d00e79a9fd095c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_203b1468863c4abc916db027e3620f54",
       "IPY_MODEL_72bde7968ed74d5093a442009b073f09",
       "IPY_MODEL_47c86d96d9c44d06919571b81c2ce05d"
      ],
      "layout": "IPY_MODEL_24c0a94e61ad417bb764aecb3ee5fe2b"
     }
    },
    "0b2c321446a043ea8d818ea73cee3467": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11a1a3515ef14371ba8bfa0337a847c9",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0912ed2603ae486594d8dedb926ad515",
      "value": "‚Äá3.08G/3.08G‚Äá[02:08&lt;00:00,‚Äá44.8MB/s]"
     }
    },
    "0e0f03f7ce164813b13f425a24fd8501": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_813bc32a56924002b1286e4688f6ad74",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c6143dae7327459da1a131b492b315e6",
      "value": "‚Äá800/800‚Äá[00:02&lt;00:00,‚Äá379.95‚Äáexamples/s]"
     }
    },
    "10a4a3fcd0814d4b8da40c5739c6801b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11a1a3515ef14371ba8bfa0337a847c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "127d77e3e1cf44ae9ca30d8b76c62731": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12aa8da20401480bb1441996e93b73e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12ff7069a95045fa89e714e7c1a252c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef40352f9cbd4bff942a5f013a8fa37e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b5542b19477d47ac9c4fb432b30b2572",
      "value": "Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=2):‚Äá100%"
     }
    },
    "1482448779a845fcb29c11ead21824af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce3140122c0e4b7bbeacec98c5a7fbbd",
       "IPY_MODEL_e8a18ecde8ad4f75a585dd94714863ce",
       "IPY_MODEL_7c53cf7391e34fa193f885c756bc1dc9"
      ],
      "layout": "IPY_MODEL_a373ddfb2e78472d8f3c66a1dfeeaf7f"
     }
    },
    "16fe4989505d4eeb884c3f355178d66a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30401891f7ce41a2859eb7b707f90d9a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8a36ef377a8f4bb0a089bdd039aba10f",
      "value": "Map:‚Äá100%"
     }
    },
    "1a8635ad89384d95a8d4089cd019ee8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c89f0c204fa41f5b87ece81a83dd7f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "203b1468863c4abc916db027e3620f54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6199a6cfdcd648f9ac612c4d6d3521ca",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_966e73bfc01f43cfbd217cff383c7b13",
      "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
     }
    },
    "22ddc39f38a143ab9016838928147eb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe6ec67f959042beb462c0c3c4423bcd",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a162d07200f84d9cb1bdee7ea811e4be",
      "value": "model-00003-of-00004.safetensors:‚Äá100%"
     }
    },
    "24c0a94e61ad417bb764aecb3ee5fe2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28b0303ead8c4c7798b31a3a8de105e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "296af5276675463bbb03ebceb6a4e1ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e74f01ec40944af8a337a759a8d22b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "30401891f7ce41a2859eb7b707f90d9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3389a3c0c48047ef8b48878f91b2d3ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2ca46eb05d743a29c6fc739f81b0612",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0771e36286134f6190d6cc28099059ec",
      "value": "‚Äá4.97G/4.97G‚Äá[02:23&lt;00:00,‚Äá31.9MB/s]"
     }
    },
    "38ed62e3ff524ed8bf77f83c2376902e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10a4a3fcd0814d4b8da40c5739c6801b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4f559079472149c9b7e0e771608a3242",
      "value": "‚Äá800/800‚Äá[00:17&lt;00:00,‚Äá61.98‚Äáexamples/s]"
     }
    },
    "390f11e412684a9dbfd9c083778a656d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8b5a35f9e44480aaf3c652c0cf6eddf",
      "max": 800,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_80cd84b111bc4c8b87276fa159316ec4",
      "value": 800
     }
    },
    "42b0336aa45e4a9e985ec883c12f6d29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "433f85434f46469090b2e3f0ec3ca195": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47c86d96d9c44d06919571b81c2ce05d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f570074fc7674ac89f216097f373b40e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d26f438f685b4c3794ba0bbce885f704",
      "value": "‚Äá3/3‚Äá[00:55&lt;00:00,‚Äá16.55s/it]"
     }
    },
    "491103c2b7a741f0bf5683e8b0568147": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bc509a9da134d3b8146831f3f7d57b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4d3f3bad427f4e749e11a99c2091c10c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a039d3f04a0f4733906363ff472f03d7",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_94f18a827258417aa11b9c8703b1ed4b",
      "value": "Map‚Äá(num_proc=2):‚Äá100%"
     }
    },
    "4f559079472149c9b7e0e771608a3242": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "521f5a4dc2c946c8860ae11a7002b94c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ad938e318fa2430690b5e6aa36d40421",
       "IPY_MODEL_9cc6a4df88884feeb8fd913e9fe6404e",
       "IPY_MODEL_3389a3c0c48047ef8b48878f91b2d3ec"
      ],
      "layout": "IPY_MODEL_1c89f0c204fa41f5b87ece81a83dd7f4"
     }
    },
    "5650913486bd462d8e1f074e0140e1bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59d89f209b6a4f77bfb874c280ee4e0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c397e3dd58346459602e9cfbb3c3a33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5d754e8c18fc49308866ae0b33199f1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6084a19541a74cb2b8277d419086ceb4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6199a6cfdcd648f9ac612c4d6d3521ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63b8524926374892b21d46f9c8796265": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6e9171196e9b47c8962cc652395a8cc9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "72bde7968ed74d5093a442009b073f09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_127d77e3e1cf44ae9ca30d8b76c62731",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9123a238550948f2a9645ca825535360",
      "value": 3
     }
    },
    "796cc39f8c4e49b89ee34b7985979d69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7aba1346734a4bb2a6d6b9b432663dc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c53cf7391e34fa193f885c756bc1dc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_296af5276675463bbb03ebceb6a4e1ee",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_bc9eeb7a833245328a60e15dc7970ec9",
      "value": "‚Äá171k/?‚Äá[00:00&lt;00:00,‚Äá3.91MB/s]"
     }
    },
    "80cd84b111bc4c8b87276fa159316ec4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "813bc32a56924002b1286e4688f6ad74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "826965fde298476cb440f3826dca1276": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82ed345043e94b87aa58052712844536": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83aa9a3274414b56840ab851d543cdc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8a36ef377a8f4bb0a089bdd039aba10f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8bcacb40eae14d1596352deb18328a99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f38593d098b42d08ba4b25039eb4baa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0380ac8639cc4f5d9fb8949a9a3a62b9",
      "max": 3077103576,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fb087ed37640404aa43c48be53fe4670",
      "value": 3077103576
     }
    },
    "8fbdd308c6364c99815be62a3d1685fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7aba1346734a4bb2a6d6b9b432663dc6",
      "max": 800,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ce4458cbf7d64adc82d8c132cac20a70",
      "value": 800
     }
    },
    "9123a238550948f2a9645ca825535360": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "94f18a827258417aa11b9c8703b1ed4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "966e73bfc01f43cfbd217cff383c7b13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96aac8cc27cd4dbd9557588b0b106ba0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_16fe4989505d4eeb884c3f355178d66a",
       "IPY_MODEL_390f11e412684a9dbfd9c083778a656d",
       "IPY_MODEL_0e0f03f7ce164813b13f425a24fd8501"
      ],
      "layout": "IPY_MODEL_12aa8da20401480bb1441996e93b73e6"
     }
    },
    "9cc6a4df88884feeb8fd913e9fe6404e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82ed345043e94b87aa58052712844536",
      "max": 4966792808,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5c397e3dd58346459602e9cfbb3c3a33",
      "value": 4966792808
     }
    },
    "a039d3f04a0f4733906363ff472f03d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a046e9afa6e44799949fd7b47d2f4d7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4d3f3bad427f4e749e11a99c2091c10c",
       "IPY_MODEL_fdd00d01732d48f891bf58370f779976",
       "IPY_MODEL_d15e825af73b4cf5b5e47640c5b0f863"
      ],
      "layout": "IPY_MODEL_5d754e8c18fc49308866ae0b33199f1f"
     }
    },
    "a0c5fd5bcf0941f3ab3a8bb1e136913c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a162d07200f84d9cb1bdee7ea811e4be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a373ddfb2e78472d8f3c66a1dfeeaf7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a59f8eb15348418daeae816305d66250": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_12ff7069a95045fa89e714e7c1a252c1",
       "IPY_MODEL_8fbdd308c6364c99815be62a3d1685fd",
       "IPY_MODEL_38ed62e3ff524ed8bf77f83c2376902e"
      ],
      "layout": "IPY_MODEL_5650913486bd462d8e1f074e0140e1bc"
     }
    },
    "a8b5a35f9e44480aaf3c652c0cf6eddf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad938e318fa2430690b5e6aa36d40421": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_491103c2b7a741f0bf5683e8b0568147",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8bcacb40eae14d1596352deb18328a99",
      "value": "model-00002-of-00004.safetensors:‚Äá100%"
     }
    },
    "afeff137435944a196a228705096f5dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28b0303ead8c4c7798b31a3a8de105e2",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2e74f01ec40944af8a337a759a8d22b3",
      "value": "model-00001-of-00004.safetensors:‚Äá100%"
     }
    },
    "b5542b19477d47ac9c4fb432b30b2572": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b80346a6e19541f4958caf12aa1cb501": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_796cc39f8c4e49b89ee34b7985979d69",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_42b0336aa45e4a9e985ec883c12f6d29",
      "value": "‚Äá4.99G/4.99G‚Äá[02:21&lt;00:00,‚Äá188MB/s]"
     }
    },
    "bc9eeb7a833245328a60e15dc7970ec9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c173ebd47e6c40d48c1defcd9455feaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_22ddc39f38a143ab9016838928147eb2",
       "IPY_MODEL_c960d0168a30445797d1098ebeefa603",
       "IPY_MODEL_b80346a6e19541f4958caf12aa1cb501"
      ],
      "layout": "IPY_MODEL_826965fde298476cb440f3826dca1276"
     }
    },
    "c6143dae7327459da1a131b492b315e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c75a11c3a6b4467ea482288eed8a15a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_afeff137435944a196a228705096f5dd",
       "IPY_MODEL_8f38593d098b42d08ba4b25039eb4baa",
       "IPY_MODEL_0b2c321446a043ea8d818ea73cee3467"
      ],
      "layout": "IPY_MODEL_433f85434f46469090b2e3f0ec3ca195"
     }
    },
    "c960d0168a30445797d1098ebeefa603": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e703a10606ac44a48061ef33a21f7b64",
      "max": 4992870216,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_63b8524926374892b21d46f9c8796265",
      "value": 4992870216
     }
    },
    "ce3140122c0e4b7bbeacec98c5a7fbbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59d89f209b6a4f77bfb874c280ee4e0b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a0c5fd5bcf0941f3ab3a8bb1e136913c",
      "value": "model.safetensors.index.json:‚Äá"
     }
    },
    "ce4458cbf7d64adc82d8c132cac20a70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d15e825af73b4cf5b5e47640c5b0f863": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6084a19541a74cb2b8277d419086ceb4",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_decbefd3caaa4308834722f9b757598e",
      "value": "‚Äá800/800‚Äá[00:10&lt;00:00,‚Äá108.99‚Äáexamples/s]"
     }
    },
    "d26f438f685b4c3794ba0bbce885f704": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "decbefd3caaa4308834722f9b757598e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e2ca46eb05d743a29c6fc739f81b0612": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e703a10606ac44a48061ef33a21f7b64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8a18ecde8ad4f75a585dd94714863ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e9171196e9b47c8962cc652395a8cc9",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4bc509a9da134d3b8146831f3f7d57b2",
      "value": 1
     }
    },
    "ef40352f9cbd4bff942a5f013a8fa37e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f570074fc7674ac89f216097f373b40e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb087ed37640404aa43c48be53fe4670": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fdd00d01732d48f891bf58370f779976": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a8635ad89384d95a8d4089cd019ee8f",
      "max": 800,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_83aa9a3274414b56840ab851d543cdc3",
      "value": 800
     }
    },
    "fe6ec67f959042beb462c0c3c4423bcd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
