{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Gemma 3N D-Fire Dataset Inference & Evaluation\n",
        "\n",
        "This notebook focuses on inference using Gemma 3N to evaluate fire and smoke detection on the D-Fire dataset. \n",
        "\n",
        "**Key Features:**\n",
        "- ðŸ”¥ **Fire/Smoke Detection**: Evaluate Gemma 3N on D-Fire dataset\n",
        "- ðŸ“Š **Comprehensive Evaluation**: Multiple prompt types and evaluation metrics\n",
        "- ðŸŽ¯ **Structured Testing**: Similar to main.py evaluation approach\n",
        "- ðŸ“ˆ **Performance Analysis**: Detailed metrics and analysis\n",
        "\n",
        "**Configuration:**\n",
        "- Multiple prompt types for comprehensive evaluation\n",
        "- Configurable dataset splits (train/test)\n",
        "- Performance metrics and analysis\n",
        "- Results export and visualization\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Installation & Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    %pip install unsloth\n",
        "else:\n",
        "    # Colab installation\n",
        "    %pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    %pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    %pip install --no-deps unsloth\n",
        "\n",
        "# Install latest transformers for Gemma 3N\n",
        "%pip install --no-deps git+https://github.com/huggingface/transformers.git\n",
        "%pip install --no-deps --upgrade timm\n",
        "\n",
        "# Additional packages for evaluation\n",
        "%pip install scikit-learn matplotlib seaborn tqdm\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Mount Google Drive & Dataset Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Verify D-Fire dataset structure\n",
        "import os\n",
        "DATASET_PATH = \"/content/drive/MyDrive/D-Fire\"\n",
        "\n",
        "print(\"Checking D-Fire dataset structure...\")\n",
        "if os.path.exists(DATASET_PATH):\n",
        "    print(f\"âœ… Found D-Fire folder at: {DATASET_PATH}\")\n",
        "    \n",
        "    for split in [\"train\", \"test\"]:\n",
        "        split_path = os.path.join(DATASET_PATH, split)\n",
        "        if os.path.exists(split_path):\n",
        "            images_path = os.path.join(split_path, \"images\")\n",
        "            labels_path = os.path.join(split_path, \"labels\")\n",
        "            if os.path.exists(images_path) and os.path.exists(labels_path):\n",
        "                print(f\"âœ… {split}: {len(os.listdir(images_path))} images, {len(os.listdir(labels_path))} labels\")\n",
        "            else:\n",
        "                print(f\"âŒ Missing {split}/images or {split}/labels\")\n",
        "        else:\n",
        "            print(f\"âŒ {split} folder not found\")\n",
        "else:\n",
        "    print(f\"âŒ Dataset not found at: {DATASET_PATH}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Load Gemma 3N Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "# Load Gemma 3N model for inference\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3n-E4B-it\",\n",
        "    dtype = None,  # Auto detection\n",
        "    max_seq_length = 1024,\n",
        "    load_in_4bit = True,\n",
        "    # token = \"hf_...\",  # Add if using gated models\n",
        ")\n",
        "\n",
        "print(\"âœ… Model loaded successfully!\")\n",
        "print(f\"Model: {model.config.name_or_path}\")\n",
        "print(f\"Device: {next(model.parameters()).device}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Configuration & Prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation Configuration (similar to main.py)\n",
        "DETECTION_PROMPTS = {\n",
        "    \"simple\": \"Is there fire or smoke visible in this image? Answer with: 'fire', 'smoke', 'both', or 'none'.\",\n",
        "    \"detailed\": \"Analyze this image carefully for fire or smoke detection. Look for flames, smoke clouds, or signs of burning. Respond with exactly one word: 'fire' if you see flames, 'smoke' if you see smoke without flames, 'both' if you see both fire and smoke, or 'none' if neither is present.\",\n",
        "    \"context\": \"You are a fire detection system. Your task is to identify fire and smoke in images. Examine this image and classify what you see. Answer with only: 'fire', 'smoke', 'both', or 'none'.\"\n",
        "}\n",
        "\n",
        "# Category mappings\n",
        "CATEGORY_LABELS = {\n",
        "    \"fire\": \"fire\",\n",
        "    \"smoke\": \"smoke\", \n",
        "    \"both\": \"both\",\n",
        "    \"none\": \"none\"\n",
        "}\n",
        "\n",
        "# Inference settings (recommended for Gemma 3N)\n",
        "INFERENCE_SETTINGS = {\n",
        "    \"temperature\": 1.0,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 64,\n",
        "    \"max_new_tokens\": 16\n",
        "}\n",
        "\n",
        "# Setup session paths and logging (similar to main.py)\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Create session name\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "models_str = \"gemma-3n-E4B-it\"\n",
        "prompts_str = \"_\".join(list(DETECTION_PROMPTS.keys())[:2])  # Use first 2 prompt types for name\n",
        "session_name = f\"{timestamp}_{models_str}_{prompts_str}_inference\"\n",
        "\n",
        "# Create directory structure\n",
        "results_base = Path(\"results\")\n",
        "session_path = results_base / session_name\n",
        "evals_path = session_path / \"evals\"\n",
        "plots_path = session_path / \"plots\"\n",
        "logs_path = Path(\"logs\") / f\"logs_{session_name}\"\n",
        "\n",
        "# Create directories\n",
        "session_path.mkdir(parents=True, exist_ok=True)\n",
        "evals_path.mkdir(parents=True, exist_ok=True)\n",
        "plots_path.mkdir(parents=True, exist_ok=True)\n",
        "logs_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Setup logging\n",
        "log_file = logs_path / \"evaluation.log\"\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file),\n",
        "        logging.StreamHandler()  # Also log to console\n",
        "    ]\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(\"GemmaEvaluationNotebook\")\n",
        "\n",
        "print(\"âœ… Configuration loaded:\")\n",
        "print(f\"Prompt types: {list(DETECTION_PROMPTS.keys())}\")\n",
        "print(f\"Category labels: {list(CATEGORY_LABELS.keys())}\")\n",
        "print(f\"ðŸ“ Session folder: {session_path}\")\n",
        "print(f\"ðŸ“ Logs will be saved to: {log_file}\")\n",
        "\n",
        "logger.info(\"=\" * 80)\n",
        "logger.info(\"GEMMA 3N D-FIRE INFERENCE SESSION STARTED\")\n",
        "logger.info(\"=\" * 80)\n",
        "logger.info(f\"Session folder: {session_path}\")\n",
        "logger.info(f\"Inference settings: {INFERENCE_SETTINGS}\")\n",
        "logger.info(f\"Available prompts: {list(DETECTION_PROMPTS.keys())}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Dataset Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "class DFireEvaluationDataset:\n",
        "    \"\"\"D-Fire dataset loader for evaluation (similar to main.py's approach).\"\"\"\n",
        "    \n",
        "    def __init__(self, dataset_path: str):\n",
        "        self.dataset_path = Path(dataset_path)\n",
        "        self.train_path = self.dataset_path / \"train\"\n",
        "        self.test_path = self.dataset_path / \"test\"\n",
        "        \n",
        "    def _load_yolo_labels(self, labels_dir: Path) -> dict:\n",
        "        \"\"\"Load YOLO format labels.\"\"\"\n",
        "        labels = {}\n",
        "        for label_file in labels_dir.glob(\"*.txt\"):\n",
        "            image_name = label_file.stem\n",
        "            annotations = []\n",
        "            \n",
        "            if label_file.stat().st_size > 0:\n",
        "                with open(label_file, 'r') as f:\n",
        "                    for line in f:\n",
        "                        parts = line.strip().split()\n",
        "                        if len(parts) >= 5:\n",
        "                            class_id = int(parts[0])\n",
        "                            annotations.append({\n",
        "                                'class_id': class_id,\n",
        "                                'class_name': 'smoke' if class_id == 0 else 'fire'\n",
        "                            })\n",
        "            labels[image_name] = annotations\n",
        "        return labels\n",
        "    \n",
        "    def _categorize_images(self, labels: dict) -> dict:\n",
        "        \"\"\"Categorize images based on annotations.\"\"\"\n",
        "        categories = {'fire': [], 'smoke': [], 'both': [], 'none': []}\n",
        "        \n",
        "        for image_name, annotations in labels.items():\n",
        "            if not annotations:\n",
        "                categories['none'].append(image_name)\n",
        "            else:\n",
        "                has_fire = any(ann['class_name'] == 'fire' for ann in annotations)\n",
        "                has_smoke = any(ann['class_name'] == 'smoke' for ann in annotations)\n",
        "                \n",
        "                if has_fire and has_smoke:\n",
        "                    categories['both'].append(image_name)\n",
        "                elif has_fire:\n",
        "                    categories['fire'].append(image_name)\n",
        "                elif has_smoke:\n",
        "                    categories['smoke'].append(image_name)\n",
        "                else:\n",
        "                    categories['none'].append(image_name)\n",
        "        \n",
        "        return categories\n",
        "    \n",
        "    def get_sample_images(self, split: str = \"test\", max_per_category: int = 50, seed: int = 42) -> list:\n",
        "        \"\"\"Get sample images for evaluation.\"\"\"\n",
        "        random.seed(seed)\n",
        "        \n",
        "        split_path = self.test_path if split == \"test\" else self.train_path\n",
        "        images_dir = split_path / \"images\"\n",
        "        labels_dir = split_path / \"labels\"\n",
        "        \n",
        "        if not images_dir.exists() or not labels_dir.exists():\n",
        "            raise FileNotFoundError(f\"Images or labels directory not found for {split} split\")\n",
        "        \n",
        "        # Load labels and categorize\n",
        "        labels = self._load_yolo_labels(labels_dir)\n",
        "        categories = self._categorize_images(labels)\n",
        "        \n",
        "        # Sample images from each category\n",
        "        sample_images = []\n",
        "        for category, image_names in categories.items():\n",
        "            if max_per_category and len(image_names) > max_per_category:\n",
        "                selected_images = random.sample(image_names, max_per_category)\n",
        "            else:\n",
        "                selected_images = image_names\n",
        "            \n",
        "            for image_name in selected_images:\n",
        "                image_path = images_dir / f\"{image_name}.jpg\"\n",
        "                if image_path.exists():\n",
        "                    sample_images.append({\n",
        "                        'image_name': f\"{image_name}.jpg\",\n",
        "                        'image_path': str(image_path),\n",
        "                        'category': category,\n",
        "                        'label': category,\n",
        "                        'split': split\n",
        "                    })\n",
        "        \n",
        "        return sample_images\n",
        "    \n",
        "    def get_dataset_stats(self, split: str = \"test\") -> dict:\n",
        "        \"\"\"Get dataset statistics.\"\"\"\n",
        "        split_path = self.test_path if split == \"test\" else self.train_path\n",
        "        images_dir = split_path / \"images\"\n",
        "        labels_dir = split_path / \"labels\"\n",
        "        \n",
        "        if not images_dir.exists() or not labels_dir.exists():\n",
        "            return {'total_images': 0, 'categories': {}, 'annotations': {'total': 0, 'fire': 0, 'smoke': 0}}\n",
        "        \n",
        "        labels = self._load_yolo_labels(labels_dir)\n",
        "        categories = self._categorize_images(labels)\n",
        "        \n",
        "        # Count annotations\n",
        "        total_annotations = 0\n",
        "        fire_annotations = 0\n",
        "        smoke_annotations = 0\n",
        "        \n",
        "        for annotations in labels.values():\n",
        "            total_annotations += len(annotations)\n",
        "            fire_annotations += sum(1 for ann in annotations if ann['class_name'] == 'fire')\n",
        "            smoke_annotations += sum(1 for ann in annotations if ann['class_name'] == 'smoke')\n",
        "        \n",
        "        total_images = sum(len(images) for images in categories.values())\n",
        "        \n",
        "        return {\n",
        "            'total_images': total_images,\n",
        "            'categories': {cat: len(images) for cat, images in categories.items()},\n",
        "            'annotations': {\n",
        "                'total': total_annotations,\n",
        "                'fire': fire_annotations,\n",
        "                'smoke': smoke_annotations\n",
        "            }\n",
        "        }\n",
        "\n",
        "# Initialize dataset\n",
        "dataset = DFireEvaluationDataset(DATASET_PATH)\n",
        "print(\"âœ… Dataset loader initialized\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Dataset Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display dataset statistics\n",
        "def display_dataset_stats(stats, split_name):\n",
        "    print(f\"\\nðŸ“Š {split_name.upper()} Split Statistics:\")\n",
        "    print(f\"Total Images: {stats['total_images']}\")\n",
        "    print(\"\\nCategories:\")\n",
        "    for category, count in stats['categories'].items():\n",
        "        percentage = (count / stats['total_images'] * 100) if stats['total_images'] > 0 else 0\n",
        "        print(f\"  {category.title()}: {count} ({percentage:.1f}%)\")\n",
        "    \n",
        "    print(\"\\nAnnotations:\")\n",
        "    ann_stats = stats['annotations']\n",
        "    print(f\"  Fire: {ann_stats['fire']}\")\n",
        "    print(f\"  Smoke: {ann_stats['smoke']}\")\n",
        "    print(f\"  Total: {ann_stats['total']}\")\n",
        "\n",
        "# Get and display statistics for both splits\n",
        "test_stats = dataset.get_dataset_stats(\"test\")\n",
        "train_stats = dataset.get_dataset_stats(\"train\")\n",
        "\n",
        "display_dataset_stats(test_stats, \"test\")\n",
        "display_dataset_stats(train_stats, \"train\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Inference Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple and clean version - just extract between model turn markers\n",
        "def evaluate_single_image_simple(image_path: str, prompt: str, verbose: bool = False) -> dict:\n",
        "    \"\"\"Evaluate a single image with the model - SIMPLE VERSION.\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Prepare messages\n",
        "        messages = [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image_path},\n",
        "                {\"type\": \"text\", \"text\": prompt}\n",
        "            ]\n",
        "        }]\n",
        "        \n",
        "        # Apply chat template and generate\n",
        "        outputs = model.generate(\n",
        "            **tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                add_generation_prompt=True,\n",
        "                tokenize=True,\n",
        "                return_dict=True,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(\"cuda\"),\n",
        "            max_new_tokens=INFERENCE_SETTINGS[\"max_new_tokens\"],\n",
        "            temperature=INFERENCE_SETTINGS[\"temperature\"],\n",
        "            top_p=INFERENCE_SETTINGS[\"top_p\"],\n",
        "            top_k=INFERENCE_SETTINGS[\"top_k\"],\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        \n",
        "        # Decode response\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract model response - split by lines and find the last meaningful line\n",
        "        lines = response.strip().split('\\n')\n",
        "        \n",
        "        # Find the last line that contains just the model response\n",
        "        answer = \"\"\n",
        "        for i in range(len(lines) - 1, -1, -1):  # Go backwards through lines\n",
        "            line = lines[i].strip()\n",
        "            if line and line.lower() in [\"fire\", \"smoke\", \"both\", \"none\"]:\n",
        "                answer = line.lower()\n",
        "                break\n",
        "        \n",
        "        # If no direct match found, try to find after \"model\" marker\n",
        "        if not answer:\n",
        "            # Look for \"model\" marker and get the next non-empty line\n",
        "            for i, line in enumerate(lines):\n",
        "                if line.strip() == \"model\" and i + 1 < len(lines):\n",
        "                    next_line = lines[i + 1].strip().lower()\n",
        "                    if next_line in [\"fire\", \"smoke\", \"both\", \"none\"]:\n",
        "                        answer = next_line\n",
        "                        break\n",
        "        \n",
        "        # Final fallback - if still no answer, default to \"none\"\n",
        "        if not answer:\n",
        "            answer = \"none\"\n",
        "        \n",
        "        predicted_label = answer\n",
        "        \n",
        "        processing_time = time.time() - start_time\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"Full response: {response}\")\n",
        "            print(f\"Extracted answer: '{answer}'\")\n",
        "            print(f\"Predicted: {predicted_label}\")\n",
        "        \n",
        "        return {\n",
        "            \"predicted_label\": predicted_label,\n",
        "            \"raw_response\": answer,\n",
        "            \"processing_time\": processing_time,\n",
        "            \"error\": None\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"predicted_label\": \"none\",\n",
        "            \"raw_response\": \"\",\n",
        "            \"processing_time\": 0,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# Override the original function\n",
        "evaluate_single_image = evaluate_single_image_simple\n",
        "\n",
        "print(\"âœ… Simple inference function loaded\")\n",
        "print(\"ðŸ”§ Now extracts model response between <start_of_turn>model and <end_of_turn>\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the new simple parsing logic\n",
        "def test_simple_parsing():\n",
        "    \"\"\"Test the simple line-based parsing logic.\"\"\"\n",
        "    print(\"ðŸ§ª Testing the simple parsing logic:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Test cases based on actual model responses\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"raw_response\": \"user\\n\\nis there fire or smoke visible in this image? answer with: 'fire', 'smoke', 'both', or 'none'.\\nmodel\\nfire\",\n",
        "            \"expected_answer\": \"fire\"\n",
        "        },\n",
        "        {\n",
        "            \"raw_response\": \"user\\n\\nis there fire or smoke visible in this image? answer with: 'fire', 'smoke', 'both', or 'none'.\\nmodel\\nboth\",\n",
        "            \"expected_answer\": \"both\"\n",
        "        },\n",
        "        {\n",
        "            \"raw_response\": \"user\\n\\nis there fire or smoke visible in this image? answer with: 'fire', 'smoke', 'both', or 'none'.\\nmodel\\nsmoke\",\n",
        "            \"expected_answer\": \"smoke\"\n",
        "        },\n",
        "        {\n",
        "            \"raw_response\": \"user\\n\\nis there fire or smoke visible in this image? answer with: 'fire', 'smoke', 'both', or 'none'.\\nmodel\\nnone\",\n",
        "            \"expected_answer\": \"none\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    for i, test_case in enumerate(test_cases, 1):\n",
        "        print(f\"\\nTest {i}: Expected '{test_case['expected_answer']}'\")\n",
        "        \n",
        "        # Apply the new parsing logic\n",
        "        response = test_case['raw_response']\n",
        "        lines = response.strip().split('\\n')\n",
        "        \n",
        "        # Find the last line that contains just the model response\n",
        "        answer = \"\"\n",
        "        for j in range(len(lines) - 1, -1, -1):  # Go backwards through lines\n",
        "            line = lines[j].strip()\n",
        "            if line and line.lower() in [\"fire\", \"smoke\", \"both\", \"none\"]:\n",
        "                answer = line.lower()\n",
        "                break\n",
        "        \n",
        "        # If no direct match found, try to find after \"model\" marker\n",
        "        if not answer:\n",
        "            # Look for \"model\" marker and get the next non-empty line\n",
        "            for j, line in enumerate(lines):\n",
        "                if line.strip() == \"model\" and j + 1 < len(lines):\n",
        "                    next_line = lines[j + 1].strip().lower()\n",
        "                    if next_line in [\"fire\", \"smoke\", \"both\", \"none\"]:\n",
        "                        answer = next_line\n",
        "                        break\n",
        "        \n",
        "        # Final fallback\n",
        "        if not answer:\n",
        "            answer = \"none\"\n",
        "        \n",
        "        print(f\"  Raw response lines: {lines}\")\n",
        "        print(f\"  Extracted answer: '{answer}'\")\n",
        "        print(f\"  Result: {'âœ…' if answer == test_case['expected_answer'] else 'âŒ'}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"ðŸŽ¯ The simple logic correctly extracts single-word responses!\")\n",
        "    print(\"ðŸ”§ Key approach:\")\n",
        "    print(\"   - Split response into lines\")\n",
        "    print(\"   - Look backwards for lines containing valid responses\")\n",
        "    print(\"   - Fall back to looking after 'model' marker\")\n",
        "\n",
        "test_simple_parsing()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "import time\n",
        "\n",
        "# Setup chat template\n",
        "tokenizer = get_chat_template(tokenizer, chat_template=\"gemma-3\")\n",
        "\n",
        "def evaluate_single_image(image_path: str, prompt: str, verbose: bool = False) -> dict:\n",
        "    \"\"\"Evaluate a single image with the model.\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Prepare messages\n",
        "        messages = [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image_path},\n",
        "                {\"type\": \"text\", \"text\": prompt}\n",
        "            ]\n",
        "        }]\n",
        "        \n",
        "        # Apply chat template and generate - Use the same approach as do_gemma_3n_inference\n",
        "        # This fixes the \"Number of images does not match number of special image tokens\" error\n",
        "        # by ensuring the chat template properly handles image tokens\n",
        "        outputs = model.generate(\n",
        "            **tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                add_generation_prompt=True,\n",
        "                tokenize=True,\n",
        "                return_dict=True,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(\"cuda\"),\n",
        "            max_new_tokens=INFERENCE_SETTINGS[\"max_new_tokens\"],\n",
        "            temperature=INFERENCE_SETTINGS[\"temperature\"],\n",
        "            top_p=INFERENCE_SETTINGS[\"top_p\"],\n",
        "            top_k=INFERENCE_SETTINGS[\"top_k\"],\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        \n",
        "        # Decode response\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract the model's answer (after the last model turn)\n",
        "        if \"<start_of_turn>model\" in response:\n",
        "            answer = response.split(\"<start_of_turn>model\")[-1].strip()\n",
        "        else:\n",
        "            answer = response.strip()\n",
        "        \n",
        "        # Clean up the answer\n",
        "        answer = answer.lower().strip()\n",
        "        \n",
        "        # Map to valid categories\n",
        "        if \"fire\" in answer and \"smoke\" in answer:\n",
        "            predicted_label = \"both\"\n",
        "        elif \"fire\" in answer:\n",
        "            predicted_label = \"fire\"\n",
        "        elif \"smoke\" in answer:\n",
        "            predicted_label = \"smoke\"\n",
        "        elif \"none\" in answer:\n",
        "            predicted_label = \"none\"\n",
        "        else:\n",
        "            predicted_label = \"none\"  # Default fallback\n",
        "        \n",
        "        processing_time = time.time() - start_time\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"Raw response: {answer}\")\n",
        "            print(f\"Predicted: {predicted_label}\")\n",
        "        \n",
        "        return {\n",
        "            \"predicted_label\": predicted_label,\n",
        "            \"raw_response\": answer,\n",
        "            \"processing_time\": processing_time,\n",
        "            \"error\": None\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"predicted_label\": \"none\",\n",
        "            \"raw_response\": \"\",\n",
        "            \"processing_time\": 0,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "def evaluate_images_batch(sample_images: list, prompt_type: str, verbose: bool = False) -> list:\n",
        "    \"\"\"Evaluate a batch of images.\"\"\"\n",
        "    results = []\n",
        "    prompt = DETECTION_PROMPTS[prompt_type]\n",
        "    \n",
        "    print(f\"\\nðŸ” Evaluating {len(sample_images)} images with '{prompt_type}' prompt...\")\n",
        "    \n",
        "    for i, sample in enumerate(sample_images):\n",
        "        if verbose or (i + 1) % 10 == 0:\n",
        "            print(f\"Processing image {i+1}/{len(sample_images)}: {sample['image_name']}\")\n",
        "        \n",
        "        # Evaluate image\n",
        "        eval_result = evaluate_single_image(sample['image_path'], prompt, verbose=verbose)\n",
        "        \n",
        "        # Combine with sample info\n",
        "        result = {\n",
        "            **sample,\n",
        "            **eval_result,\n",
        "            \"prompt_type\": prompt_type,\n",
        "            \"prompt\": prompt,\n",
        "            \"model\": \"gemma-3n-E4B-it\"\n",
        "        }\n",
        "        \n",
        "        results.append(result)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"  Expected: {sample['label']} | Predicted: {eval_result['predicted_label']}\")\n",
        "            print(f\"  Time: {eval_result['processing_time']:.2f}s\")\n",
        "            if eval_result['error']:\n",
        "                print(f\"  Error: {eval_result['error']}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"âœ… Inference functions loaded\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Load Sample Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for sampling\n",
        "MAX_IMAGES_PER_CATEGORY = 20  # Adjust based on your needs\n",
        "DATASET_SPLIT = \"test\"  # or \"train\"\n",
        "\n",
        "# Sample images\n",
        "print(f\"ðŸ“¸ Sampling {MAX_IMAGES_PER_CATEGORY} images per category from {DATASET_SPLIT} split...\")\n",
        "logger.info(f\"Sampling {MAX_IMAGES_PER_CATEGORY} images per category from {DATASET_SPLIT} split\")\n",
        "\n",
        "sample_images = dataset.get_sample_images(\n",
        "    split=DATASET_SPLIT,\n",
        "    max_per_category=MAX_IMAGES_PER_CATEGORY,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(f\"âœ… Selected {len(sample_images)} images for evaluation\")\n",
        "logger.info(f\"Selected {len(sample_images)} images for evaluation\")\n",
        "\n",
        "# Display sample distribution\n",
        "from collections import Counter\n",
        "category_counts = Counter(img['category'] for img in sample_images)\n",
        "print(\"\\nSample distribution:\")\n",
        "for category, count in category_counts.items():\n",
        "    print(f\"  {category.title()}: {count} images\")\n",
        "    logger.info(f\"  {category.title()}: {count} images\")\n",
        "\n",
        "logger.info(f\"Dataset split: {DATASET_SPLIT}\")\n",
        "logger.info(f\"Max images per category: {MAX_IMAGES_PER_CATEGORY}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Run Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation with different prompt types\n",
        "all_results = []\n",
        "\n",
        "# Select prompt types to evaluate\n",
        "PROMPT_TYPES_TO_EVALUATE = [\"simple\", \"detailed\"]  # Add \"context\" if needed\n",
        "\n",
        "logger.info(\"Starting evaluation process...\")\n",
        "logger.info(f\"Prompt types to evaluate: {PROMPT_TYPES_TO_EVALUATE}\")\n",
        "\n",
        "for prompt_type in PROMPT_TYPES_TO_EVALUATE:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ðŸš€ Starting evaluation with '{prompt_type}' prompt\")\n",
        "    print(f\"{'='*60}\")\n",
        "    logger.info(f\"Starting evaluation with {prompt_type} prompts\")\n",
        "    \n",
        "    # Evaluate with current prompt type\n",
        "    results = evaluate_images_batch(\n",
        "        sample_images, \n",
        "        prompt_type, \n",
        "        verbose=False  # Set to True for detailed output\n",
        "    )\n",
        "    \n",
        "    all_results.extend(results)\n",
        "    \n",
        "    # Quick summary for this prompt type\n",
        "    correct = sum(1 for r in results if r['label'] == r['predicted_label'] and not r['error'])\n",
        "    total = len(results)\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    \n",
        "    print(f\"\\nðŸ“Š {prompt_type.title()} Results:\")\n",
        "    print(f\"  Accuracy: {accuracy:.3f} ({correct}/{total})\")\n",
        "    print(f\"  Errors: {sum(1 for r in results if r['error'])}\")\n",
        "    \n",
        "    logger.info(f\"Completed evaluation with {prompt_type} prompts. Got {len(results)} results.\")\n",
        "    logger.info(f\"{prompt_type} Accuracy: {accuracy:.3f} ({correct}/{total})\")\n",
        "\n",
        "print(f\"\\nâœ… Evaluation complete! Total results: {len(all_results)}\")\n",
        "logger.info(\"Evaluation process completed\")\n",
        "logger.info(f\"Total evaluation results: {len(all_results)}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Performance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def analyze_results(results: list) -> dict:\n",
        "    \"\"\"Analyze evaluation results.\"\"\"\n",
        "    # Filter successful results\n",
        "    successful_results = [r for r in results if not r['error']]\n",
        "    \n",
        "    if not successful_results:\n",
        "        print(\"âŒ No successful results to analyze\")\n",
        "        return {}\n",
        "    \n",
        "    # Extract labels and predictions\n",
        "    true_labels = [r['label'] for r in successful_results]\n",
        "    predicted_labels = [r['predicted_label'] for r in successful_results]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "    \n",
        "    # Processing time stats\n",
        "    processing_times = [r['processing_time'] for r in successful_results]\n",
        "    avg_processing_time = np.mean(processing_times)\n",
        "    \n",
        "    # Classification report\n",
        "    class_report = classification_report(\n",
        "        true_labels, predicted_labels, \n",
        "        target_names=['fire', 'smoke', 'both', 'none'],\n",
        "        zero_division=0,\n",
        "        output_dict=True\n",
        "    )\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predicted_labels, \n",
        "                         labels=['fire', 'smoke', 'both', 'none'])\n",
        "    \n",
        "    analysis = {\n",
        "        'total_evaluations': len(results),\n",
        "        'successful_evaluations': len(successful_results),\n",
        "        'error_rate': (len(results) - len(successful_results)) / len(results),\n",
        "        'accuracy': accuracy,\n",
        "        'f1_score': f1,\n",
        "        'average_processing_time': avg_processing_time,\n",
        "        'classification_report': class_report,\n",
        "        'confusion_matrix': cm,\n",
        "        'true_labels': true_labels,\n",
        "        'predicted_labels': predicted_labels\n",
        "    }\n",
        "    \n",
        "    return analysis\n",
        "\n",
        "def display_analysis(analysis: dict, title: str = \"Performance Analysis\", plots_dir: str = None):\n",
        "    \"\"\"Display analysis results and save plots.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ðŸ“ˆ {title}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    print(f\"Total Evaluations: {analysis['total_evaluations']}\")\n",
        "    print(f\"Successful Evaluations: {analysis['successful_evaluations']}\")\n",
        "    print(f\"Error Rate: {analysis['error_rate']:.3f}\")\n",
        "    print(f\"Accuracy: {analysis['accuracy']:.3f}\")\n",
        "    print(f\"F1-Score: {analysis['f1_score']:.3f}\")\n",
        "    print(f\"Average Processing Time: {analysis['average_processing_time']:.2f}s\")\n",
        "    \n",
        "    # Display classification report\n",
        "    print(\"\\nðŸ“Š Classification Report:\")\n",
        "    class_report = analysis['classification_report']\n",
        "    \n",
        "    for label in ['fire', 'smoke', 'both', 'none']:\n",
        "        if label in class_report:\n",
        "            metrics = class_report[label]\n",
        "            print(f\"  {label.title()}: P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1-score']:.3f}\")\n",
        "    \n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(analysis['confusion_matrix'], \n",
        "                annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['fire', 'smoke', 'both', 'none'],\n",
        "                yticklabels=['fire', 'smoke', 'both', 'none'])\n",
        "    plt.title(f'Confusion Matrix - {title}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save plot if plots_dir is provided\n",
        "    if plots_dir:\n",
        "        plot_filename = title.lower().replace(' ', '_').replace('-', '_') + '_confusion_matrix.png'\n",
        "        plot_path = Path(plots_dir) / plot_filename\n",
        "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"ðŸ“ˆ Plot saved to: {plot_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def generate_performance_plots(all_results: list, plots_dir: str):\n",
        "    \"\"\"Generate comprehensive performance plots and save them.\"\"\"\n",
        "    plots_dir = Path(plots_dir)\n",
        "    plots_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # 1. Performance by prompt type\n",
        "    if len(PROMPT_TYPES_TO_EVALUATE) > 1:\n",
        "        prompt_metrics = {}\n",
        "        for prompt_type in PROMPT_TYPES_TO_EVALUATE:\n",
        "            prompt_results = [r for r in all_results if r['prompt_type'] == prompt_type]\n",
        "            if prompt_results:\n",
        "                prompt_analysis = analyze_results(prompt_results)\n",
        "                prompt_metrics[prompt_type] = {\n",
        "                    'accuracy': prompt_analysis['accuracy'],\n",
        "                    'f1_score': prompt_analysis['f1_score'],\n",
        "                    'processing_time': prompt_analysis['average_processing_time']\n",
        "                }\n",
        "        \n",
        "        if prompt_metrics:\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "            \n",
        "            # Accuracy comparison\n",
        "            prompts = list(prompt_metrics.keys())\n",
        "            accuracies = [prompt_metrics[p]['accuracy'] for p in prompts]\n",
        "            axes[0].bar(prompts, accuracies, color='skyblue')\n",
        "            axes[0].set_title('Accuracy by Prompt Type')\n",
        "            axes[0].set_ylabel('Accuracy')\n",
        "            axes[0].set_ylim(0, 1)\n",
        "            for i, v in enumerate(accuracies):\n",
        "                axes[0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
        "            \n",
        "            # F1-Score comparison\n",
        "            f1_scores = [prompt_metrics[p]['f1_score'] for p in prompts]\n",
        "            axes[1].bar(prompts, f1_scores, color='lightgreen')\n",
        "            axes[1].set_title('F1-Score by Prompt Type')\n",
        "            axes[1].set_ylabel('F1-Score')\n",
        "            axes[1].set_ylim(0, 1)\n",
        "            for i, v in enumerate(f1_scores):\n",
        "                axes[1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
        "            \n",
        "            # Processing time comparison\n",
        "            times = [prompt_metrics[p]['processing_time'] for p in prompts]\n",
        "            axes[2].bar(prompts, times, color='lightcoral')\n",
        "            axes[2].set_title('Processing Time by Prompt Type')\n",
        "            axes[2].set_ylabel('Time (seconds)')\n",
        "            for i, v in enumerate(times):\n",
        "                axes[2].text(i, v + 0.1, f'{v:.2f}s', ha='center')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plot_path = plots_dir / \"prompt_comparison.png\"\n",
        "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"ðŸ“ˆ Prompt comparison plot saved to: {plot_path}\")\n",
        "            plt.show()\n",
        "    \n",
        "    # 2. Processing time distribution\n",
        "    processing_times = [r['processing_time'] for r in all_results if not r['error']]\n",
        "    if processing_times:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist(processing_times, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        plt.title('Processing Time Distribution')\n",
        "        plt.xlabel('Processing Time (seconds)')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.axvline(np.mean(processing_times), color='red', linestyle='--', \n",
        "                   label=f'Mean: {np.mean(processing_times):.2f}s')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plot_path = plots_dir / \"processing_time_distribution.png\"\n",
        "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"ðŸ“ˆ Processing time distribution saved to: {plot_path}\")\n",
        "        plt.show()\n",
        "\n",
        "# Note: plots_dir will be set when we create the directory structure\n",
        "# For now, just run the analysis\n",
        "print(\"ðŸ“Š Running performance analysis...\")\n",
        "\n",
        "# Analyze overall results\n",
        "overall_analysis = analyze_results(all_results)\n",
        "display_analysis(overall_analysis, \"Overall Performance\")\n",
        "\n",
        "# Analyze by prompt type\n",
        "for prompt_type in PROMPT_TYPES_TO_EVALUATE:\n",
        "    prompt_results = [r for r in all_results if r['prompt_type'] == prompt_type]\n",
        "    prompt_analysis = analyze_results(prompt_results)\n",
        "    display_analysis(prompt_analysis, f\"Performance - {prompt_type.title()} Prompt\")\n",
        "\n",
        "print(\"âœ… Performance analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to files (using pre-configured paths)\n",
        "import json\n",
        "\n",
        "print(f\"ðŸ’¾ Saving results to session folder: {session_path}\")\n",
        "logger.info(\"Starting results saving process...\")\n",
        "logger.info(f\"Saving to session: {session_path}\")\n",
        "\n",
        "# Save raw results to evals folder\n",
        "results_file = evals_path / \"raw_results.json\"\n",
        "with open(results_file, 'w') as f:\n",
        "    json.dump(all_results, f, indent=2, default=str)\n",
        "\n",
        "print(f\"ðŸ’¾ Raw results saved to: {results_file}\")\n",
        "logger.info(f\"Raw results saved to: {results_file}\")\n",
        "\n",
        "# Save analysis summary to evals folder\n",
        "analysis_file = evals_path / \"evaluation_report.json\"\n",
        "analysis_summary = {\n",
        "    'evaluation_summary': {\n",
        "        'total_evaluations': len(all_results),\n",
        "        'successful_evaluations': overall_analysis['successful_evaluations'],\n",
        "        'error_rate': overall_analysis['error_rate'],\n",
        "        'models_evaluated': ['gemma-3n-E4B-it'],\n",
        "        'prompt_types_used': PROMPT_TYPES_TO_EVALUATE,\n",
        "    },\n",
        "    'dataset_statistics': {\n",
        "        'test': test_stats,\n",
        "        'train': train_stats\n",
        "    },\n",
        "    'performance_metrics': {\n",
        "        'total_evaluations': len(all_results),\n",
        "        'successful_evaluations': overall_analysis['successful_evaluations'],\n",
        "        'error_rate': overall_analysis['error_rate'],\n",
        "        'average_processing_time': overall_analysis['average_processing_time'],\n",
        "        'classification': {\n",
        "            'accuracy': overall_analysis['accuracy'],\n",
        "            'f1_score': overall_analysis['f1_score'],\n",
        "            'precision': overall_analysis['classification_report'].get('weighted avg', {}).get('precision', 0),\n",
        "            'recall': overall_analysis['classification_report'].get('weighted avg', {}).get('recall', 0)\n",
        "        }\n",
        "    },\n",
        "    'configuration': {\n",
        "        'model': 'gemma-3n-E4B-it',\n",
        "        'dataset_split': DATASET_SPLIT,\n",
        "        'max_images_per_category': MAX_IMAGES_PER_CATEGORY,\n",
        "        'prompt_types': PROMPT_TYPES_TO_EVALUATE,\n",
        "        'inference_settings': INFERENCE_SETTINGS\n",
        "    },\n",
        "    'timestamp': timestamp\n",
        "}\n",
        "\n",
        "with open(analysis_file, 'w') as f:\n",
        "    json.dump(analysis_summary, f, indent=2, default=str)\n",
        "\n",
        "print(f\"ðŸ“Š Analysis report saved to: {analysis_file}\")\n",
        "logger.info(f\"Analysis report saved to: {analysis_file}\")\n",
        "\n",
        "# Save human-readable summary\n",
        "summary_file = evals_path / \"evaluation_report.txt\"\n",
        "with open(summary_file, 'w') as f:\n",
        "    f.write(\"=\" * 80 + \"\\n\")\n",
        "    f.write(\"GEMMA 3N FIRE/SMOKE DETECTION EVALUATION REPORT\\n\")\n",
        "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "    \n",
        "    # Summary\n",
        "    f.write(\"EVALUATION SUMMARY\\n\")\n",
        "    f.write(\"-\" * 20 + \"\\n\")\n",
        "    f.write(f\"Total Evaluations: {len(all_results)}\\n\")\n",
        "    f.write(f\"Successful Evaluations: {overall_analysis['successful_evaluations']}\\n\")\n",
        "    f.write(f\"Error Rate: {overall_analysis['error_rate']:.2%}\\n\")\n",
        "    f.write(f\"Model Evaluated: gemma-3n-E4B-it\\n\")\n",
        "    f.write(f\"Prompt Types: {', '.join(PROMPT_TYPES_TO_EVALUATE)}\\n\\n\")\n",
        "    \n",
        "    # Dataset Statistics\n",
        "    f.write(\"DATASET STATISTICS\\n\")\n",
        "    f.write(\"-\" * 20 + \"\\n\")\n",
        "    stats = test_stats if DATASET_SPLIT == 'test' else train_stats\n",
        "    f.write(f\"Total Images: {stats.get('total_images', 'N/A')}\\n\")\n",
        "    if 'categories' in stats:\n",
        "        for category, count in stats['categories'].items():\n",
        "            f.write(f\"  {category.title()}: {count}\\n\")\n",
        "    f.write(\"\\n\")\n",
        "    \n",
        "    # Performance Metrics\n",
        "    f.write(\"OVERALL PERFORMANCE\\n\")\n",
        "    f.write(\"-\" * 20 + \"\\n\")\n",
        "    f.write(f\"Accuracy: {overall_analysis['accuracy']:.3f}\\n\")\n",
        "    f.write(f\"F1-Score: {overall_analysis['f1_score']:.3f}\\n\")\n",
        "    f.write(f\"Average Processing Time: {overall_analysis['average_processing_time']:.2f}s\\n\")\n",
        "    \n",
        "    # Per-prompt performance\n",
        "    f.write(f\"\\nPer-Prompt Performance:\\n\")\n",
        "    f.write(\"=\" * 30 + \"\\n\")\n",
        "    for prompt_type in PROMPT_TYPES_TO_EVALUATE:\n",
        "        prompt_results = [r for r in all_results if r['prompt_type'] == prompt_type]\n",
        "        if prompt_results:\n",
        "            prompt_analysis = analyze_results(prompt_results)\n",
        "            f.write(f\"\\n{prompt_type.title()} Prompt:\\n\")\n",
        "            f.write(f\"  Accuracy: {prompt_analysis['accuracy']:.3f}\\n\")\n",
        "            f.write(f\"  F1-Score: {prompt_analysis['f1_score']:.3f}\\n\")\n",
        "            f.write(f\"  Evaluations: {prompt_analysis['successful_evaluations']}\\n\")\n",
        "    \n",
        "    f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "    f.write(\"Report generated by Gemma 3N D-Fire Inference Notebook\\n\")\n",
        "    f.write(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(f\"ðŸ“‹ Human-readable summary saved to: {summary_file}\")\n",
        "logger.info(f\"Human-readable summary saved to: {summary_file}\")\n",
        "\n",
        "# Generate and save plots to plots folder\n",
        "print(\"\\nðŸ“ˆ Generating and saving plots...\")\n",
        "generate_performance_plots(all_results, plots_path)\n",
        "\n",
        "# Save confusion matrices with plots_dir\n",
        "for prompt_type in PROMPT_TYPES_TO_EVALUATE:\n",
        "    prompt_results = [r for r in all_results if r['prompt_type'] == prompt_type]\n",
        "    if prompt_results:\n",
        "        prompt_analysis = analyze_results(prompt_results)\n",
        "        # Save confusion matrix for this prompt type\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(prompt_analysis['confusion_matrix'], \n",
        "                    annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['fire', 'smoke', 'both', 'none'],\n",
        "                    yticklabels=['fire', 'smoke', 'both', 'none'])\n",
        "        plt.title(f'Confusion Matrix - {prompt_type.title()} Prompt')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        plot_path = plots_path / f\"{prompt_type}_confusion_matrix.png\"\n",
        "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"ðŸ“ˆ {prompt_type.title()} confusion matrix saved to: {plot_path}\")\n",
        "        plt.close()  # Close to avoid displaying in notebook\n",
        "\n",
        "# Save overall confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(overall_analysis['confusion_matrix'], \n",
        "            annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['fire', 'smoke', 'both', 'none'],\n",
        "            yticklabels=['fire', 'smoke', 'both', 'none'])\n",
        "plt.title('Overall Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "\n",
        "overall_plot_path = plots_path / \"overall_confusion_matrix.png\"\n",
        "plt.savefig(overall_plot_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"ðŸ“ˆ Overall confusion matrix saved to: {overall_plot_path}\")\n",
        "plt.close()\n",
        "\n",
        "# Display key metrics\n",
        "print(f\"\\nðŸŽ¯ Key Results:\")\n",
        "print(f\"  Overall Accuracy: {overall_analysis['accuracy']:.3f}\")\n",
        "print(f\"  Overall F1-Score: {overall_analysis['f1_score']:.3f}\")\n",
        "print(f\"  Average Processing Time: {overall_analysis['average_processing_time']:.2f}s\")\n",
        "print(f\"  Total Images Evaluated: {overall_analysis['successful_evaluations']}\")\n",
        "print(f\"  Error Rate: {overall_analysis['error_rate']:.3f}\")\n",
        "\n",
        "print(f\"\\nðŸ“ All results saved to session folder: {session_path}\")\n",
        "print(f\"   ðŸ“Š Evaluations: {evals_path}\")\n",
        "print(f\"   ðŸ“ˆ Plots: {plots_path}\")\n",
        "print(f\"   ðŸ“ Logs: {logs_path}\")\n",
        "\n",
        "logger.info(\"=\" * 80)\n",
        "logger.info(\"EVALUATION SESSION COMPLETED SUCCESSFULLY\")\n",
        "logger.info(\"=\" * 80)\n",
        "logger.info(f\"Overall accuracy: {overall_analysis['accuracy']:.3f}\")\n",
        "logger.info(f\"Overall F1-score: {overall_analysis['f1_score']:.3f}\")\n",
        "logger.info(f\"Results saved to: {session_path}\")\n",
        "logger.info(f\"Raw results: {results_file}\")\n",
        "logger.info(f\"Analysis report: {analysis_file}\")\n",
        "logger.info(f\"Plots generated: {plots_path}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Evaluation pipeline completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test Individual Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test individual images with detailed output\n",
        "print(\"\\nðŸ” Testing Individual Images:\")\n",
        "\n",
        "# Select a few samples from each category\n",
        "test_samples = []\n",
        "for category in ['fire', 'smoke', 'both', 'none']:\n",
        "    category_samples = [img for img in sample_images if img['category'] == category]\n",
        "    if category_samples:\n",
        "        test_samples.append(category_samples[0])  # Take first sample from each category\n",
        "\n",
        "# Test with simple prompt\n",
        "prompt = DETECTION_PROMPTS['simple']\n",
        "\n",
        "for i, sample in enumerate(test_samples):\n",
        "    print(f\"\\n--- Test {i+1} ---\")\n",
        "    print(f\"Image: {sample['image_name']}\")\n",
        "    print(f\"Category: {sample['category']}\")\n",
        "    print(f\"Expected: {sample['label']}\")\n",
        "    \n",
        "    # Evaluate image\n",
        "    result = evaluate_single_image(sample['image_path'], prompt, verbose=True)\n",
        "    \n",
        "    # Show result\n",
        "    print(f\"Predicted: {result['predicted_label']}\")\n",
        "    print(f\"Correct: {'âœ…' if result['predicted_label'] == sample['label'] else 'âŒ'}\")\n",
        "    print(f\"Processing Time: {result['processing_time']:.2f}s\")\n",
        "    \n",
        "    if result['error']:\n",
        "        print(f\"Error: {result['error']}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
